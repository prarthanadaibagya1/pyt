{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnRE9yJ9HCBk"
      },
      "source": [
        "**LAB : 06**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GfTxD7uG-YD"
      },
      "source": [
        "Advanced Data Wrangling and Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OBJECTIVES:\n",
        "-to learn data wrangling and transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "THEORY:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data wrangling (data munging) is the process of transforming raw, messy, incomplete, or inconsistent data into a clean and structured format suitable for analysis.\n",
        "    Purpose of data wrangling:\n",
        "        Improve data quality\n",
        "        Make data analysis-ready\n",
        "        Integrate multiple data sources\n",
        "        Reduce errors in models\n",
        "In real-world scenarios, data collected from different sources is often incomplete, inconsistent, and noisy. Before performing analysis or building machine learning models, the data must be cleaned and transformed. This lab focused on practical implementation of various data preprocessing techniques such as handling missing values, removing duplicates, normalization, and restructuring datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1LdHTLdGlkT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WFBDh_pH1wO",
        "outputId": "1ca8839e-435a-4309-a155-525c129a364b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sales_transactions.csv created successfully!\n"
          ]
        }
      ],
      "source": [
        "sales_data = {\n",
        "    \"transaction_id\": [\"TX001\", \"TX002\", \"TX003\", \"TX004\", \"TX005\"],\n",
        "    \"customer_id\": [\"CUST101\", \"CUST102\", \"CUST103\", \"CUST101\", \"CUST104\"],\n",
        "    \"product_id\": [\"PROD201\", \"PROD202\", \"PROD201\", \"PROD203\", \"PROD204\"],\n",
        "    \"store_id\": [\"ST01\", \"ST02\", \"ST01\", \"ST03\", \"ST02\"],\n",
        "    \"quantity\": [3, 1, 2, 5, 1],\n",
        "    \"sale_date\": [\"2024-01-15\", \"2024-01-15\", \"2024-01-16\", \"2024-01-16\", \"2024-01-17\"]\n",
        "}\n",
        "\n",
        "sales_df = pd.DataFrame(sales_data)\n",
        "\n",
        "sales_df.to_csv(\"sales_transactions.csv\", index=False)\n",
        "\n",
        "print(\"sales_transactions.csv created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImputwTpH9Vu",
        "outputId": "cb76a40e-d862-4dec-9242-d80f6a51bee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "customers.csv created successfully!\n"
          ]
        }
      ],
      "source": [
        "customers_data = {\n",
        "    \"customer_id\": [\"CUST101\", \"CUST102\", \"CUST105\"],\n",
        "    \"name\": [\"Alice Brown\", \"Bob Smith\", \"Carol White\"],\n",
        "    \"email\": [\"alice@email.com\", \"bob@email.com\", \"carol@email.com\"],\n",
        "    \"city\": [\"New York\", \"London\", \"Tokyo\"],\n",
        "    \"country\": [\"USA\", \"UK\", \"Japan\"],\n",
        "    \"join_date\": [\"2023-03-15\", \"2023-05-20\", \"2023-07-10\"],\n",
        "    \"loyalty_tier\": [\"Gold\", \"Silver\", \"Gold\"]\n",
        "}\n",
        "\n",
        "customers_df = pd.DataFrame(customers_data)\n",
        "\n",
        "customers_df.to_csv(\"customers.csv\", index=False)\n",
        "\n",
        "print(\"customers.csv created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOpR6kdwIEOj",
        "outputId": "797f7dec-110d-4305-90b2-aeb232de0d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "products.csv created successfully!\n"
          ]
        }
      ],
      "source": [
        "products_data = {\n",
        "    \"product_id\": [\"PROD201\", \"PROD202\", \"PROD203\", \"PROD205\"],\n",
        "    \"product_name\": [\"Laptop\", \"Coffee Maker\", \"Office Chair\", \"Headphones\"],\n",
        "    \"category\": [\"Electronics\", \"Appliances\", \"Furniture\", \"Electronics\"],\n",
        "    \"unit_price\": [1200.00, 89.99, 250.00, 150.00],\n",
        "    \"supplier_id\": [\"SUP01\", \"SUP02\", \"SUP01\", \"SUP03\"],\n",
        "    \"stock_qty\": [50, 100, 75, 200]\n",
        "}\n",
        "\n",
        "products_df = pd.DataFrame(products_data)\n",
        "\n",
        "products_df.to_csv(\"products.csv\", index=False)\n",
        "\n",
        "print(\"products.csv created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQgDqz-4INPn",
        "outputId": "211abd5a-777d-44cd-a3aa-8a7369e6fcd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Product Info:\n",
            "   transaction_id customer_id product_id store_id  quantity   sale_date name  \\\n",
            "4          TX005     CUST104    PROD204     ST02         1  2024-01-17  NaN   \n",
            "\n",
            "  email city country join_date loyalty_tier product_name category  unit_price  \\\n",
            "4   NaN  NaN     NaN       NaN          NaN          NaN      NaN         NaN   \n",
            "\n",
            "  supplier_id  stock_qty  \n",
            "4         NaN        NaN  \n",
            "\n",
            "Revenue per Loyalty Tier:\n",
            " loyalty_tier\n",
            "Gold      4850.00\n",
            "Silver      89.99\n",
            "Name: revenue, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "sales_df = pd.read_csv(\"sales_transactions.csv\")\n",
        "customers_df = pd.read_csv(\"customers.csv\")\n",
        "products_df = pd.read_csv(\"products.csv\")\n",
        "\n",
        "\n",
        "sales_customers = pd.merge(sales_df, customers_df,\n",
        "                           on=\"customer_id\",\n",
        "                           how=\"left\")\n",
        "\n",
        "# ii. Merge with products (keep all transactions)\n",
        "full_data = pd.merge(sales_customers, products_df,\n",
        "                     on=\"product_id\",\n",
        "                     how=\"left\")\n",
        "\n",
        "# iii. Find transactions with missing product info\n",
        "missing_products = full_data[full_data[\"product_name\"].isna()]\n",
        "print(\"Missing Product Info:\\n\", missing_products)\n",
        "\n",
        "# iv. Calculate revenue\n",
        "full_data[\"revenue\"] = full_data[\"quantity\"] * full_data[\"unit_price\"]\n",
        "\n",
        "# Group revenue by loyalty tier\n",
        "revenue_per_segment = full_data.groupby(\"loyalty_tier\")[\"revenue\"].sum()\n",
        "\n",
        "print(\"\\nRevenue per Loyalty Tier:\\n\", revenue_per_segment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d25VwkDuI-Jx",
        "outputId": "797b41ca-1c1f-4d86-a079-63c3bb290b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate Transaction IDs:\n",
            " Empty DataFrame\n",
            "Columns: [transaction_id, customer_id, product_id, store_id, quantity, sale_date, name, email, city, country, join_date, loyalty_tier, product_name, category, unit_price, supplier_id, stock_qty, revenue]\n",
            "Index: []\n",
            "\n",
            "Exact Duplicate Rows:\n",
            " Empty DataFrame\n",
            "Columns: [transaction_id, customer_id, product_id, store_id, quantity, sale_date, name, email, city, country, join_date, loyalty_tier, product_name, category, unit_price, supplier_id, stock_qty, revenue]\n",
            "Index: []\n",
            "\n",
            "Duplicates with Different Values:\n",
            " Empty DataFrame\n",
            "Columns: [transaction_id, customer_id, product_id, store_id, quantity, sale_date, name, email, city, country, join_date, loyalty_tier, product_name, category, unit_price, supplier_id, stock_qty, revenue]\n",
            "Index: []\n",
            "\n",
            "Data after removing duplicate transaction IDs:\n",
            "   transaction_id customer_id product_id store_id  quantity   sale_date  \\\n",
            "0          TX001     CUST101    PROD201     ST01         3  2024-01-15   \n",
            "1          TX002     CUST102    PROD202     ST02         1  2024-01-15   \n",
            "2          TX003     CUST103    PROD201     ST01         2  2024-01-16   \n",
            "3          TX004     CUST101    PROD203     ST03         5  2024-01-16   \n",
            "4          TX005     CUST104    PROD204     ST02         1  2024-01-17   \n",
            "\n",
            "          name            email      city country   join_date loyalty_tier  \\\n",
            "0  Alice Brown  alice@email.com  New York     USA  2023-03-15         Gold   \n",
            "1    Bob Smith    bob@email.com    London      UK  2023-05-20       Silver   \n",
            "2          NaN              NaN       NaN     NaN         NaN          NaN   \n",
            "3  Alice Brown  alice@email.com  New York     USA  2023-03-15         Gold   \n",
            "4          NaN              NaN       NaN     NaN         NaN          NaN   \n",
            "\n",
            "   product_name     category  unit_price supplier_id  stock_qty  revenue  \n",
            "0        Laptop  Electronics     1200.00       SUP01       50.0  3600.00  \n",
            "1  Coffee Maker   Appliances       89.99       SUP02      100.0    89.99  \n",
            "2        Laptop  Electronics     1200.00       SUP01       50.0  2400.00  \n",
            "3  Office Chair    Furniture      250.00       SUP01       75.0  1250.00  \n",
            "4           NaN          NaN         NaN         NaN        NaN      NaN  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assume full_data is the merged dataframe\n",
        "\n",
        "# a. Identify duplicate transaction IDs\n",
        "duplicate_ids = full_data[full_data.duplicated(\"transaction_id\", keep=False)]\n",
        "print(\"Duplicate Transaction IDs:\\n\", duplicate_ids)\n",
        "\n",
        "\n",
        "# b. Check if duplicates are exact copies\n",
        "exact_duplicates = full_data[full_data.duplicated(keep=False)]\n",
        "print(\"\\nExact Duplicate Rows:\\n\", exact_duplicates)\n",
        "\n",
        "\n",
        "# Find duplicates with same ID but different values\n",
        "non_exact_duplicates = duplicate_ids.drop_duplicates()\n",
        "print(\"\\nDuplicates with Different Values:\\n\", non_exact_duplicates)\n",
        "\n",
        "\n",
        "# c. Strategy: Keep first occurrence and remove others\n",
        "cleaned_data = full_data.drop_duplicates(subset=\"transaction_id\", keep=\"first\")\n",
        "\n",
        "print(\"\\nData after removing duplicate transaction IDs:\\n\", cleaned_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF-207CWJ5IP",
        "outputId": "5dc77c49-60a8-47da-b56c-e2e5063d33f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "product_category  Clothing  Electronics  Furniture\n",
            "region                                            \n",
            "East                7000.0          NaN        NaN\n",
            "North              17500.0      33000.0        NaN\n",
            "South                  NaN      25500.0     5000.0\n",
            "                  sales_amount                       units_sold              \\\n",
            "product_category      Clothing Electronics Furniture   Clothing Electronics   \n",
            "date       region                                                             \n",
            "2024-01-01 North        8000.0     15000.0       NaN      120.0        25.0   \n",
            "           South           NaN     12000.0    5000.0        NaN        20.0   \n",
            "2024-01-02 East         7000.0         NaN       NaN       95.0         NaN   \n",
            "           North        9500.0     18000.0       NaN      135.0        30.0   \n",
            "           South           NaN     13500.0       NaN        NaN        22.0   \n",
            "\n",
            "                             \n",
            "product_category  Furniture  \n",
            "date       region            \n",
            "2024-01-01 North        NaN  \n",
            "           South       10.0  \n",
            "2024-01-02 East         NaN  \n",
            "           North        NaN  \n",
            "           South        NaN  \n",
            "product_category  Clothing  Electronics  Furniture\n",
            "region                                            \n",
            "East                7000.0          NaN        NaN\n",
            "North              17500.0      33000.0        NaN\n",
            "South                  NaN      25500.0     5000.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"date\": [\"2024-01-01\",\"2024-01-01\",\"2024-01-01\",\"2024-01-01\",\n",
        "             \"2024-01-02\",\"2024-01-02\",\"2024-01-02\",\"2024-01-02\"],\n",
        "    \"store_id\": [\"S01\",\"S01\",\"S02\",\"S02\",\"S01\",\"S01\",\"S02\",\"S03\"],\n",
        "    \"region\": [\"North\",\"North\",\"South\",\"South\",\"North\",\"North\",\"South\",\"East\"],\n",
        "    \"product_category\": [\"Electronics\",\"Clothing\",\"Electronics\",\"Furniture\",\n",
        "                         \"Electronics\",\"Clothing\",\"Electronics\",\"Clothing\"],\n",
        "    \"sales_amount\": [15000,8000,12000,5000,18000,9500,13500,7000],\n",
        "    \"units_sold\": [25,120,20,10,30,135,22,95]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "pivot_sales = df.pivot_table(values=\"sales_amount\",\n",
        "                             index=\"region\",\n",
        "                             columns=\"product_category\",\n",
        "                             aggfunc=\"sum\")\n",
        "print(pivot_sales)\n",
        "\n",
        "pivot_multi = df.pivot_table(values=[\"sales_amount\",\"units_sold\"],\n",
        "                             index=[\"date\",\"region\"],\n",
        "                             columns=\"product_category\",\n",
        "                             aggfunc=\"sum\")\n",
        "print(pivot_multi)\n",
        "\n",
        "pivot_multiple = df.pivot_table(values=\"sales_amount\",\n",
        "                                index=\"region\",\n",
        "                                columns=\"product_category\",\n",
        "                                aggfunc=\"sum\")\n",
        "print(pivot_multiple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zppjHxhtKGub",
        "outputId": "d143450d-ea2a-498d-dda0-685e55bbc022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         date region        metric  value\n",
            "0  2024-01-01  North  sales_amount  15000\n",
            "1  2024-01-01  South  sales_amount  12000\n",
            "2  2024-01-02  North  sales_amount  18000\n",
            "3  2024-01-02  South  sales_amount  13500\n",
            "4  2024-01-01  North    units_sold     25\n",
            "5  2024-01-01  South    units_sold     20\n",
            "6  2024-01-02  North    units_sold     30\n",
            "7  2024-01-02  South    units_sold     22\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"date\": [\"2024-01-01\",\"2024-01-01\",\"2024-01-02\",\"2024-01-02\"],\n",
        "    \"region\": [\"North\",\"South\",\"North\",\"South\"],\n",
        "    \"sales_amount\": [15000,12000,18000,13500],\n",
        "    \"units_sold\": [25,20,30,22]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "melted_df = df.melt(id_vars=[\"date\",\"region\"],\n",
        "                    value_vars=[\"sales_amount\",\"units_sold\"],\n",
        "                    var_name=\"metric\",\n",
        "                    value_name=\"value\")\n",
        "\n",
        "print(melted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewaGmVB_KWmA",
        "outputId": "fc08820c-dee1-4551-8d26-53fe39a4b8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stacked:\n",
            " region  product_category\n",
            "East    Clothing             7000.0\n",
            "North   Clothing             8000.0\n",
            "        Electronics         15000.0\n",
            "South   Electronics         12000.0\n",
            "        Furniture            5000.0\n",
            "dtype: float64\n",
            "\n",
            "Unstacked:\n",
            " product_category  Clothing  Electronics  Furniture\n",
            "region                                            \n",
            "East                7000.0          NaN        NaN\n",
            "North               8000.0      15000.0        NaN\n",
            "South                  NaN      12000.0     5000.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"region\": [\"North\",\"North\",\"South\",\"South\",\"East\"],\n",
        "    \"product_category\": [\"Electronics\",\"Clothing\",\"Electronics\",\"Furniture\",\"Clothing\"],\n",
        "    \"sales_amount\": [15000,8000,12000,5000,7000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "pivot_df = df.pivot_table(values=\"sales_amount\",\n",
        "                          index=\"region\",\n",
        "                          columns=\"product_category\",\n",
        "                          aggfunc=\"sum\")\n",
        "\n",
        "\n",
        "stacked = pivot_df.stack()\n",
        "print(\"Stacked:\\n\", stacked)\n",
        "\n",
        "\n",
        "unstacked = stacked.unstack()\n",
        "print(\"\\nUnstacked:\\n\", unstacked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKEhga_aKspQ",
        "outputId": "f12e6e72-fb3c-4d98-b8db-998711e08568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crosstab:\n",
            " product_category  Clothing  Electronics  Furniture\n",
            "region                                            \n",
            "East                     1            0          0\n",
            "North                    1            1          0\n",
            "South                    0            1          1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "crosstab_df = pd.crosstab(df[\"region\"], df[\"product_category\"])\n",
        "print(\"\\nCrosstab:\\n\", crosstab_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr_NTorIK5l_",
        "outputId": "dfe6968d-fd71-4676-ac14-f6d8cec09844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-index DataFrame:\n",
            "                            product_category  sales_amount  units_sold\n",
            "date       region store_id                                           \n",
            "2024-01-01 North  S01           Electronics         15000          25\n",
            "           South  S02           Electronics         12000          20\n",
            "2024-01-02 North  S01              Clothing          9500         135\n",
            "           South  S02             Furniture          5000          10\n",
            "           East   S03              Clothing          7000          95\n",
            "\n",
            "Total Sales per Region:\n",
            " region\n",
            "East      7000\n",
            "North    24500\n",
            "South    17000\n",
            "Name: sales_amount, dtype: int64\n",
            "\n",
            "Total Units per Date:\n",
            " date\n",
            "2024-01-01     45\n",
            "2024-01-02    240\n",
            "Name: units_sold, dtype: int64\n",
            "\n",
            "North Region Sales:\n",
            "                     product_category  sales_amount  units_sold\n",
            "date       store_id                                           \n",
            "2024-01-01 S01           Electronics         15000          25\n",
            "2024-01-02 S01              Clothing          9500         135\n",
            "\n",
            "Reset Multi-index:\n",
            "          date region store_id product_category  sales_amount  units_sold\n",
            "0  2024-01-01  North      S01      Electronics         15000          25\n",
            "1  2024-01-01  South      S02      Electronics         12000          20\n",
            "2  2024-01-02  North      S01         Clothing          9500         135\n",
            "3  2024-01-02  South      S02        Furniture          5000          10\n",
            "4  2024-01-02   East      S03         Clothing          7000          95\n",
            "\n",
            "Reindexed DataFrame:\n",
            "                            product_category  sales_amount  units_sold\n",
            "region date       store_id                                           \n",
            "North  2024-01-01 S01           Electronics         15000          25\n",
            "South  2024-01-01 S02           Electronics         12000          20\n",
            "North  2024-01-02 S01              Clothing          9500         135\n",
            "South  2024-01-02 S02             Furniture          5000          10\n",
            "East   2024-01-02 S03              Clothing          7000          95\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"date\": [\"2024-01-01\",\"2024-01-01\",\"2024-01-02\",\"2024-01-02\",\"2024-01-02\"],\n",
        "    \"region\": [\"North\",\"South\",\"North\",\"South\",\"East\"],\n",
        "    \"store_id\": [\"S01\",\"S02\",\"S01\",\"S02\",\"S03\"],\n",
        "    \"product_category\": [\"Electronics\",\"Electronics\",\"Clothing\",\"Furniture\",\"Clothing\"],\n",
        "    \"sales_amount\": [15000,12000,9500,5000,7000],\n",
        "    \"units_sold\": [25,20,135,10,95]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "multi_index_df = df.set_index([\"date\",\"region\",\"store_id\"])\n",
        "print(\"Multi-index DataFrame:\\n\", multi_index_df)\n",
        "\n",
        "sales_per_region = multi_index_df.groupby(level=\"region\")[\"sales_amount\"].sum()\n",
        "print(\"\\nTotal Sales per Region:\\n\", sales_per_region)\n",
        "\n",
        "units_per_date = multi_index_df.groupby(level=\"date\")[\"units_sold\"].sum()\n",
        "print(\"\\nTotal Units per Date:\\n\", units_per_date)\n",
        "\n",
        "north_sales = multi_index_df.xs(\"North\", level=\"region\")\n",
        "print(\"\\nNorth Region Sales:\\n\", north_sales)\n",
        "\n",
        "\n",
        "reset_df = multi_index_df.reset_index()  \n",
        "print(\"\\nReset Multi-index:\\n\", reset_df)\n",
        "\n",
        "reindexed_df = reset_df.set_index([\"region\",\"date\",\"store_id\"])\n",
        "print(\"\\nReindexed DataFrame:\\n\", reindexed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AydTBJGBLGAp",
        "outputId": "964f078a-48f4-4612-ecc2-c5f41e0909aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Aggregation:\n",
            "                     sales                       units      \n",
            "                      sum     mean          std   sum count\n",
            "region category                                            \n",
            "East   Clothing      7000   7000.0          NaN    95     1\n",
            "       Electronics   9000   9000.0          NaN    15     1\n",
            "North  Clothing     17500   8750.0  1060.660172   255     2\n",
            "       Electronics  15000  15000.0          NaN    25     1\n",
            "South  Electronics  12000  12000.0          NaN    20     1\n",
            "       Furniture     5000   5000.0          NaN    10     1\n",
            "\n",
            "Custom Aggregation:\n",
            "         sales                          units                 \n",
            "          sum          mean range_func   sum count <lambda_0>\n",
            "region                                                       \n",
            "East    16000   8000.000000       2000   110     2         95\n",
            "North   32500  10833.333333       7000   280     3        135\n",
            "South   17000   8500.000000       7000    30     2         20\n",
            "\n",
            "Different Functions per Column:\n",
            "              sales               units    \n",
            "               sum          mean   max min\n",
            "category                                  \n",
            "Clothing     24500   8166.666667   135  95\n",
            "Electronics  36000  12000.000000    25  15\n",
            "Furniture     5000   5000.000000    10  10\n",
            "\n",
            "Sales Z-score (transform):\n",
            "   region     category  sales  sales_zscore\n",
            "0  North  Electronics  15000      1.000000\n",
            "1  North     Clothing   8000     -0.132453\n",
            "2  South  Electronics  12000      0.000000\n",
            "3  South    Furniture   5000           NaN\n",
            "4   East     Clothing   7000     -0.927173\n",
            "5   East  Electronics   9000     -1.000000\n",
            "6  North     Clothing   9500      1.059626\n",
            "\n",
            "Total Sales per Region (apply):\n",
            " region\n",
            "East     16000\n",
            "North    32500\n",
            "South    17000\n",
            "Name: sales, dtype: int64\n",
            "\n",
            "Filtered Groups (Total Sales > 25000):\n",
            "   region     category  sales  units  sales_zscore\n",
            "0  North  Electronics  15000     25      1.000000\n",
            "1  North     Clothing   8000    120     -0.132453\n",
            "6  North     Clothing   9500    135      1.059626\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"region\": [\"North\",\"North\",\"South\",\"South\",\"East\",\"East\",\"North\"],\n",
        "    \"category\": [\"Electronics\",\"Clothing\",\"Electronics\",\"Furniture\",\"Clothing\",\"Electronics\",\"Clothing\"],\n",
        "    \"sales\": [15000,8000,12000,5000,7000,9000,9500],\n",
        "    \"units\": [25,120,20,10,95,15,135]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "agg_df = df.groupby([\"region\",\"category\"]).agg({\n",
        "    \"sales\": [\"sum\",\"mean\",\"std\"],\n",
        "    \"units\": [\"sum\",\"count\"]\n",
        "})\n",
        "print(\"Basic Aggregation:\\n\", agg_df)\n",
        "\n",
        "# a. Custom aggregation functions\n",
        "def range_func(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "custom_agg = df.groupby(\"region\").agg({\n",
        "    \"sales\": [\"sum\",\"mean\", range_func],\n",
        "    \"units\": [\"sum\", \"count\", lambda x: x.max()]\n",
        "})\n",
        "print(\"\\nCustom Aggregation:\\n\", custom_agg)\n",
        "\n",
        "agg_diff = df.groupby(\"category\").agg({\n",
        "    \"sales\": [\"sum\",\"mean\"],\n",
        "    \"units\": [\"max\",\"min\"]\n",
        "})\n",
        "print(\"\\nDifferent Functions per Column:\\n\", agg_diff)\n",
        "\n",
        "\n",
        "df[\"sales_zscore\"] = df.groupby(\"category\")[\"sales\"].transform(lambda x: (x - x.mean())/x.std())\n",
        "\n",
        "total_sales = df.groupby(\"region\")[\"sales\"].apply(np.sum)\n",
        "\n",
        "print(\"\\nSales Z-score (transform):\\n\", df[[\"region\",\"category\",\"sales\",\"sales_zscore\"]])\n",
        "print(\"\\nTotal Sales per Region (apply):\\n\", total_sales)\n",
        "\n",
        "filtered = df.groupby(\"region\").filter(lambda x: x[\"sales\"].sum() > 25000)\n",
        "print(\"\\nFiltered Groups (Total Sales > 25000):\\n\", filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "TFn5dnRnLhCB",
        "outputId": "9fcfe951-0571-44c0-9d6c-2d3a22c0d813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of Missing Values:\n",
            " patient_id         0.000000\n",
            "age               33.333333\n",
            "gender            16.666667\n",
            "blood_pressure    33.333333\n",
            "cholesterol       33.333333\n",
            "diagnosis_date    16.666667\n",
            "treatment_cost    33.333333\n",
            "dtype: float64\n",
            "\n",
            "Missing Data Patterns:\n",
            " {'age': 'MAR (depends on patient availability or recording)', 'gender': 'MCAR (random missing)', 'blood_pressure': 'MAR (missing if patient skipped checkup)', 'cholesterol': 'MAR (depends on lab test done)', 'diagnosis_date': 'MCAR (few missing, random)', 'treatment_cost': 'MNAR (likely missing if no treatment)'}\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Columns to Impute: ['age', 'gender', 'blood_pressure', 'cholesterol', 'treatment_cost']\n",
            "Columns to Drop: []\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"patient_id\": [\"P001\",\"P002\",\"P003\",\"P004\",\"P005\",\"P006\"],\n",
        "    \"age\": [45,np.nan,62,38,np.nan,55],\n",
        "    \"gender\": [\"M\",\"F\",\"M\",np.nan,\"F\",\"M\"],\n",
        "    \"blood_pressure\": [\"120/80\",np.nan,\"140/90\",\"110/70\",np.nan,\"135/85\"],\n",
        "    \"cholesterol\": [200,np.nan,240,180,220,np.nan],\n",
        "    \"diagnosis_date\": [\"2024-01-10\",\"2024-01-11\",np.nan,\"2024-01-13\",\"2024-01-14\",\"2024-01-15\"],\n",
        "    \"treatment_cost\": [1500.0,np.nan,2300.0,np.nan,1800.0,2100.0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "missing_percent = df.isna().mean() * 100\n",
        "print(\"Percentage of Missing Values:\\n\", missing_percent)\n",
        "\n",
        "missing_patterns = {\n",
        "    \"age\": \"MAR (depends on patient availability or recording)\",\n",
        "    \"gender\": \"MCAR (random missing)\",\n",
        "    \"blood_pressure\": \"MAR (missing if patient skipped checkup)\",\n",
        "    \"cholesterol\": \"MAR (depends on lab test done)\",\n",
        "    \"diagnosis_date\": \"MCAR (few missing, random)\",\n",
        "    \"treatment_cost\": \"MNAR (likely missing if no treatment)\"\n",
        "}\n",
        "print(\"\\nMissing Data Patterns:\\n\", missing_patterns)\n",
        "\n",
        "sns.heatmap(df.isna(), cbar=False, cmap=\"viridis\")\n",
        "plt.title(\"Missing Data Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "impute_cols = [\"age\",\"gender\",\"blood_pressure\",\"cholesterol\",\"treatment_cost\"]\n",
        "drop_cols = [] \n",
        "\n",
        "print(\"\\nColumns to Impute:\", impute_cols)\n",
        "print(\"Columns to Drop:\", drop_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9n9EbD9L7nw",
        "outputId": "e3c0e79e-f02e-4856-d521-a92d795d3a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data after imputation:\n",
            "   patient_id   age gender blood_pressure  cholesterol diagnosis_date  \\\n",
            "0       P001  45.0      M         120/80        200.0     2024-01-10   \n",
            "1       P002  50.0      F         110/70        210.0     2024-01-11   \n",
            "3       P004  38.0      M         110/70        180.0     2024-01-13   \n",
            "4       P005  50.0      F         135/85        220.0     2024-01-14   \n",
            "5       P006  55.0      M         135/85        210.0     2024-01-15   \n",
            "2       P003  62.0      M         140/90        240.0            NaT   \n",
            "\n",
            "   treatment_cost  \n",
            "0          1500.0  \n",
            "1          1500.0  \n",
            "3          1500.0  \n",
            "4          1800.0  \n",
            "5          2100.0  \n",
            "2          2300.0  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3911212278.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"age\"].fillna(df[\"age\"].mean(), inplace=True)        # mean\n",
            "/tmp/ipython-input-3911212278.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"cholesterol\"].fillna(df[\"cholesterol\"].median(), inplace=True)  # median\n",
            "/tmp/ipython-input-3911212278.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"gender\"].fillna(df[\"gender\"].mode()[0], inplace=True)           # mode\n",
            "/tmp/ipython-input-3911212278.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"treatment_cost\"].fillna(method=\"ffill\", inplace=True)  # forward fill\n",
            "/tmp/ipython-input-3911212278.py:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[\"treatment_cost\"].fillna(method=\"ffill\", inplace=True)  # forward fill\n",
            "/tmp/ipython-input-3911212278.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"blood_pressure\"].fillna(method=\"bfill\", inplace=True)  # backward fill\n",
            "/tmp/ipython-input-3911212278.py:36: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[\"blood_pressure\"].fillna(method=\"bfill\", inplace=True)  # backward fill\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    \"patient_id\": [\"P001\",\"P002\",\"P003\",\"P004\",\"P005\",\"P006\"],\n",
        "    \"age\": [45,np.nan,62,38,np.nan,55],\n",
        "    \"gender\": [\"M\",\"F\",\"M\",np.nan,\"F\",\"M\"],\n",
        "    \"blood_pressure\": [\"120/80\",np.nan,\"140/90\",\"110/70\",np.nan,\"135/85\"],\n",
        "    \"cholesterol\": [200,np.nan,240,180,220,np.nan],\n",
        "    \"diagnosis_date\": [\"2024-01-10\",\"2024-01-11\",np.nan,\"2024-01-13\",\"2024-01-14\",\"2024-01-15\"],\n",
        "    \"treatment_cost\": [1500.0,np.nan,2300.0,np.nan,1800.0,2100.0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df[\"age\"].fillna(df[\"age\"].mean(), inplace=True)        \n",
        "df[\"cholesterol\"].fillna(df[\"cholesterol\"].median(), inplace=True)  \n",
        "df[\"gender\"].fillna(df[\"gender\"].mode()[0], inplace=True)           \n",
        "\n",
        "df[\"diagnosis_date\"] = pd.to_datetime(df[\"diagnosis_date\"])\n",
        "df.sort_values(\"diagnosis_date\", inplace=True)\n",
        "df[\"treatment_cost\"].fillna(method=\"ffill\", inplace=True)  \n",
        "df[\"blood_pressure\"].fillna(method=\"bfill\", inplace=True)  \n",
        "\n",
        "df[\"treatment_cost\"] = df[\"treatment_cost\"].interpolate(method=\"linear\")  \n",
        "df[\"cholesterol\"] = df[\"cholesterol\"].interpolate(method=\"polynomial\", order=2)  \n",
        "\n",
        "train = df[df[\"age\"].notna()]\n",
        "test = df[df[\"age\"].isna()]\n",
        "if not test.empty:\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(train[[\"cholesterol\",\"treatment_cost\"]], train[\"age\"])\n",
        "    df.loc[test.index, \"age\"] = lr.predict(test[[\"cholesterol\",\"treatment_cost\"]])\n",
        "\n",
        "print(\"Data after imputation:\\n\", df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2cb_mYcMP91",
        "outputId": "03b9147c-2e23-40a2-fe7a-dd5005e0a2b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data with Missing Indicators:\n",
            "   patient_id   age gender blood_pressure  cholesterol diagnosis_date  \\\n",
            "0       P001  45.0      M         120/80        200.0     2024-01-10   \n",
            "1       P002   NaN      F            NaN          NaN     2024-01-11   \n",
            "2       P003  62.0      M         140/90        240.0            NaN   \n",
            "3       P004  38.0    NaN         110/70        180.0     2024-01-13   \n",
            "4       P005   NaN      F            NaN        220.0     2024-01-14   \n",
            "5       P006  55.0      M         135/85          NaN     2024-01-15   \n",
            "\n",
            "   treatment_cost  patient_id_missing  age_missing  gender_missing  \\\n",
            "0          1500.0                   0            0               0   \n",
            "1             NaN                   0            1               0   \n",
            "2          2300.0                   0            0               0   \n",
            "3             NaN                   0            0               1   \n",
            "4          1800.0                   0            1               0   \n",
            "5          2100.0                   0            0               0   \n",
            "\n",
            "   blood_pressure_missing  cholesterol_missing  diagnosis_date_missing  \\\n",
            "0                       0                    0                       0   \n",
            "1                       1                    1                       0   \n",
            "2                       0                    0                       1   \n",
            "3                       0                    0                       0   \n",
            "4                       1                    0                       0   \n",
            "5                       0                    1                       0   \n",
            "\n",
            "   treatment_cost_missing  \n",
            "0                       0  \n",
            "1                       1  \n",
            "2                       0  \n",
            "3                       1  \n",
            "4                       0  \n",
            "5                       0  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"patient_id\": [\"P001\",\"P002\",\"P003\",\"P004\",\"P005\",\"P006\"],\n",
        "    \"age\": [45,np.nan,62,38,np.nan,55],\n",
        "    \"gender\": [\"M\",\"F\",\"M\",np.nan,\"F\",\"M\"],\n",
        "    \"blood_pressure\": [\"120/80\",np.nan,\"140/90\",\"110/70\",np.nan,\"135/85\"],\n",
        "    \"cholesterol\": [200,np.nan,240,180,220,np.nan],\n",
        "    \"diagnosis_date\": [\"2024-01-10\",\"2024-01-11\",np.nan,\"2024-01-13\",\"2024-01-14\",\"2024-01-15\"],\n",
        "    \"treatment_cost\": [1500.0,np.nan,2300.0,np.nan,1800.0,2100.0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "for col in df.columns:\n",
        "    df[col + \"_missing\"] = df[col].isna().astype(int)\n",
        "\n",
        "print(\"Data with Missing Indicators:\\n\", df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIUOjOyHMk-D",
        "outputId": "645b4bf5-4cb2-4741-abc4-d4dad1015a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory before encoding: 2338\n",
            "Memory after one-hot encoding: 2024\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "import category_encoders as ce  # for advanced encodings\n",
        "\n",
        "data = {\n",
        "    \"customer_id\": [\"C001\",\"C002\",\"C003\",\"C004\",\"C005\",\"C006\"],\n",
        "    \"education_level\": [\"High School\",\"Bachelor\",\"Master\",\"PhD\",\"Bachelor\",\"High School\"],\n",
        "    \"city\": [\"Mumbai\",\"Delhi\",\"Mumbai\",\"Bangalore\",\"Delhi\",\"Chennai\"],\n",
        "    \"income_bracket\": [\"Low\",\"Medium\",\"High\",\"High\",\"Medium\",\"Low\"],\n",
        "    \"purchase_freq\": [\"Rarely\",\"Sometimes\",\"Frequently\",\"Frequently\",\"Sometimes\",\"Rarely\"],\n",
        "    \"satisfaction\": [\"Satisfied\",\"Very Satisfied\",\"Neutral\",\"Very Satisfied\",\"Satisfied\",\"Dissatisfied\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "edu_order = [\"High School\",\"Bachelor\",\"Master\",\"PhD\"]\n",
        "income_order = [\"Low\",\"Medium\",\"High\"]\n",
        "freq_order = [\"Rarely\",\"Sometimes\",\"Frequently\"]\n",
        "satisf_order = [\"Dissatisfied\",\"Neutral\",\"Satisfied\",\"Very Satisfied\"]\n",
        "\n",
        "df[\"education_level_encoded\"] = df[\"education_level\"].map({k:i for i,k in enumerate(edu_order)})\n",
        "df[\"income_bracket_encoded\"] = df[\"income_bracket\"].map({k:i for i,k in enumerate(income_order)})\n",
        "df[\"purchase_freq_encoded\"] = df[\"purchase_freq\"].map({k:i for i,k in enumerate(freq_order)})\n",
        "df[\"satisfaction_encoded\"] = df[\"satisfaction\"].map({k:i for i,k in enumerate(satisf_order)})\n",
        "\n",
        "df_onehot = pd.get_dummies(df, columns=[\"city\"], drop_first=True)\n",
        "\n",
        "print(\"Memory before encoding:\", df.memory_usage(deep=True).sum())\n",
        "print(\"Memory after one-hot encoding:\", df_onehot.memory_usage(deep=True).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new-cell-1",
        "outputId": "14e7ceff-b92b-4c9f-a7f7-2a27ebc54754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (2.3.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (3.0.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (1.8.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (1.17.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.0.5->category_encoders) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
            "[notice] To update, run: C:\\Users\\Dell\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaQGJ79wMwRW"
      },
      "outputs": [],
      "source": [
        "freq_enc = df[\"city\"].value_counts().to_dict()\n",
        "df[\"city_freq_enc\"] = df[\"city\"].map(freq_enc)\n",
        "\n",
        "df[\"purchase_made\"] = [1,0,1,1,0,0]\n",
        "target_enc = df.groupby(\"city\")[\"purchase_made\"].mean()\n",
        "df[\"city_target_enc\"] = df[\"city\"].map(target_enc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgYQZ3s2M069"
      },
      "outputs": [],
      "source": [
        "bin_enc = ce.BinaryEncoder(cols=[\"city\"])\n",
        "df_bin = bin_enc.fit_transform(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIEnClOGNsin",
        "outputId": "2f65f38e-280d-4782-d8ff-432aca5358c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2396080622.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"volume\"].fillna(0, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"date\": [\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-05\",\"2024-01-08\"],\n",
        "    \"stock_symbol\": [\"TECH\"]*5,\n",
        "    \"open_price\": [150.5,152.0,np.nan,149.8,151.5],\n",
        "    \"close_price\": [152.3,np.nan,150.5,151.2,153.0],\n",
        "    \"volume\": [1000000,950000,np.nan,1100000,1050000],\n",
        "    \"high\": [153.0,152.8,151.0,151.5,154.2],\n",
        "    \"low\": [149.8,151.2,149.5,149.0,151.0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df.set_index(\"date\", inplace=True)\n",
        "df = df.sort_index()\n",
        "\n",
        "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
        "df = df.reindex(full_index)\n",
        "\n",
        "df[[\"open_price\",\"close_price\",\"high\",\"low\"]] = df[[\"open_price\",\"close_price\",\"high\",\"low\"]].ffill()\n",
        "df[\"volume\"].fillna(0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W7U1IT0N5MT"
      },
      "outputs": [],
      "source": [
        "df[\"close_7d_ma\"] = df[\"close_price\"].rolling(window=7).mean()\n",
        "df[\"close_30d_ma\"] = df[\"close_price\"].rolling(window=30).mean()\n",
        "\n",
        "df[\"close_lag_1d\"] = df[\"close_price\"].shift(1)\n",
        "df[\"close_lag_7d\"] = df[\"close_price\"].shift(7)\n",
        "\n",
        "df[\"pct_change\"] = df[\"close_price\"].pct_change()\n",
        "df[\"returns\"] = df[\"close_price\"].diff() / df[\"close_price\"].shift(1)\n",
        "\n",
        "df[\"close_ewma\"] = df[\"close_price\"].ewm(span=7, adjust=False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5daB5duN9iA",
        "outputId": "2bbee198-12b5-4a40-be9b-c705ae290317"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1941008522.py:9: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
            "  monthly = df[\"close_price\"].resample(\"M\").mean()\n"
          ]
        }
      ],
      "source": [
        "weekly = df[\"close_price\"].resample(\"W\").mean()\n",
        "monthly = df[\"close_price\"].resample(\"M\").mean()\n",
        "\n",
        "df = df.tz_localize(\"UTC\")         \n",
        "df = df.tz_convert(\"Asia/Kolkata\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqqu8TLo0WHJ",
        "outputId": "c00b259b-244e-4552-c034-ae6bd078eb6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        age  annual_income  credit_score  years_employed  num_purchases\n",
            "0 -1.135919      -1.094220     -1.241971       -1.006278      -0.929896\n",
            "1  0.667127       0.760390      0.639803        0.514840       0.353703\n",
            "2 -0.234396      -0.321466     -0.301084       -0.304224      -0.359408\n",
            "3  1.568650       1.533144      1.580691        1.684931       1.779924\n",
            "4 -0.865462      -0.877849     -0.677439       -0.889269      -0.844323\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = {\n",
        "    \"age\": [25,45,35,55,28],\n",
        "    \"annual_income\": [35000,95000,60000,120000,42000],\n",
        "    \"credit_score\": [650,750,700,800,680],\n",
        "    \"years_employed\": [2,15,8,25,3],\n",
        "    \"num_purchases\": [5,50,25,100,8]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_standard = scaler.fit_transform(df)\n",
        "\n",
        "scaled_standard_df = pd.DataFrame(scaled_standard, columns=df.columns)\n",
        "print(scaled_standard_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isu8fYKL1B9p",
        "outputId": "a127dbcb-1cc6-4174-af64-7e7b47a983ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        age  annual_income  credit_score  years_employed  num_purchases\n",
            "0  0.000000       0.000000      0.000000        0.000000       0.000000\n",
            "1  0.666667       0.705882      0.666667        0.565217       0.473684\n",
            "2  0.333333       0.294118      0.333333        0.260870       0.210526\n",
            "3  1.000000       1.000000      1.000000        1.000000       1.000000\n",
            "4  0.100000       0.082353      0.200000        0.043478       0.031579\n"
          ]
        }
      ],
      "source": [
        "#MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_minmax = scaler.fit_transform(df)\n",
        "\n",
        "scaled_minmax_df = pd.DataFrame(scaled_minmax, columns=df.columns)\n",
        "print(scaled_minmax_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AE76pB61NiY",
        "outputId": "3754d48d-4207-478b-f864-2d3fdeb57ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        age  annual_income  credit_score  years_employed  num_purchases\n",
            "0 -0.588235      -0.471698     -0.714286       -0.500000      -0.476190\n",
            "1  0.588235       0.660377      0.714286        0.583333       0.595238\n",
            "2  0.000000       0.000000      0.000000        0.000000       0.000000\n",
            "3  1.176471       1.132075      1.428571        1.416667       1.785714\n",
            "4 -0.411765      -0.339623     -0.285714       -0.416667      -0.404762\n"
          ]
        }
      ],
      "source": [
        "#RobustScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_robust = scaler.fit_transform(df)\n",
        "\n",
        "scaled_robust_df = pd.DataFrame(scaled_robust, columns=df.columns)\n",
        "print(scaled_robust_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZqv-kRf1V6k",
        "outputId": "48e19033-eb1f-46e7-82d1-71a264adaa2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        age  annual_income  credit_score  years_employed  num_purchases\n",
            "0  0.454545       0.291667        0.8125            0.08           0.05\n",
            "1  0.818182       0.791667        0.9375            0.60           0.50\n",
            "2  0.636364       0.500000        0.8750            0.32           0.25\n",
            "3  1.000000       1.000000        1.0000            1.00           1.00\n",
            "4  0.509091       0.350000        0.8500            0.12           0.08\n"
          ]
        }
      ],
      "source": [
        "#MaxAbsScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_maxabs = scaler.fit_transform(df)\n",
        "\n",
        "scaled_maxabs_df = pd.DataFrame(scaled_maxabs, columns=df.columns)\n",
        "print(scaled_maxabs_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-7BnLO3Y1llC"
      },
      "outputs": [],
      "source": [
        "#Adding Synthetic Outliers\n",
        "df_outlier = df.copy()\n",
        "df_outlier.loc[5] = [30, 500000, 720, 5, 500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7mBuyftS2Knn"
      },
      "outputs": [],
      "source": [
        "#You are building a predictive model for customer churn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0CebDUl919mc"
      },
      "outputs": [],
      "source": [
        "#Temporal Features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataframe\n",
        "df = pd.DataFrame({\n",
        "    \"signup_date\": pd.to_datetime([\"2022-01-01\",\"2021-06-15\",\"2023-03-10\"]),\n",
        "    \"last_login\": pd.to_datetime([\"2023-01-10\",\"2023-01-01\",\"2023-01-12\"]),\n",
        "    \"age\":[25,45,35],\n",
        "    \"monthly_charges\":[50,100,70],\n",
        "    \"total_charges\":[600,3000,840],\n",
        "    \"num_support_calls\":[2,10,4],\n",
        "    \"contract_length\":[12,24,12],\n",
        "    \"plan_type\":[\"Basic\",\"Premium\",\"Basic\"]\n",
        "})\n",
        "\n",
        "today = pd.to_datetime(\"2023-01-15\")\n",
        "\n",
        "# Account age\n",
        "df[\"account_age_days\"] = (today - df[\"signup_date\"]).dt.days\n",
        "\n",
        "# Days since last login\n",
        "df[\"days_since_last_login\"] = (today - df[\"last_login\"]).dt.days\n",
        "\n",
        "# Date components\n",
        "df[\"signup_month\"] = df[\"signup_date\"].dt.month\n",
        "df[\"signup_quarter\"] = df[\"signup_date\"].dt.quarter\n",
        "df[\"signup_dayofweek\"] = df[\"signup_date\"].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xRAuodQz2M4f"
      },
      "outputs": [],
      "source": [
        "#Ratio Features\n",
        "\n",
        "# Average monthly spend\n",
        "df[\"avg_monthly_spend\"] = df[\"total_charges\"] / df[\"contract_length\"]\n",
        "\n",
        "# Support calls per month\n",
        "df[\"calls_per_month\"] = df[\"num_support_calls\"] / df[\"contract_length\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zn6KKXl72UhO"
      },
      "outputs": [],
      "source": [
        "#Binning / Discretization\n",
        "\n",
        "# Age groups\n",
        "df[\"age_group\"] = pd.cut(df[\"age\"],\n",
        "                         bins=[0,30,50,100],\n",
        "                         labels=[\"Young\",\"Middle-aged\",\"Senior\"])\n",
        "\n",
        "# Charge categories\n",
        "df[\"charge_category\"] = pd.cut(df[\"monthly_charges\"],\n",
        "                               bins=3,\n",
        "                               labels=[\"Low\",\"Medium\",\"High\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BKnSgU9m2aEn"
      },
      "outputs": [],
      "source": [
        "#Interaction Feature\n",
        "df[\"plan_contract_interaction\"] = df[\"plan_type\"] + \"_\" + df[\"contract_length\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CRuz9llz2nyI"
      },
      "outputs": [],
      "source": [
        "#Log Transformation\n",
        "import numpy as np\n",
        "\n",
        "df[\"log_total_charges\"] = np.log1p(df[\"total_charges\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vZ_ELJPq2rDW"
      },
      "outputs": [],
      "source": [
        "#Box-Cox Transformation\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "boxcox = PowerTransformer(method='box-cox')\n",
        "df[\"boxcox_total\"] = boxcox.fit_transform(df[[\"total_charges\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8bzvj5nx2zGi"
      },
      "outputs": [],
      "source": [
        "#Yeo-Jhonson Transformation\n",
        "yeojohnson = PowerTransformer(method='yeo-johnson')\n",
        "df[\"yj_support_calls\"] = yeojohnson.fit_transform(df[[\"num_support_calls\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "aqSwhCKy25ZV",
        "outputId": "2ddbbd1b-05c8-4685-d16e-c47e2ba85969"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Visualizing Distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(df[\"total_charges\"])\n",
        "plt.title(\"Before Transformation\")\n",
        "plt.show()\n",
        "\n",
        "plt.hist(df[\"log_total_charges\"])\n",
        "plt.title(\"After Log Transformation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wJa9XlFz3EHO"
      },
      "outputs": [],
      "source": [
        "#Polynomial Features (Degree 2)\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "poly_features = poly.fit_transform(df[[\"monthly_charges\",\"age\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ieVOsUBO3I4c"
      },
      "outputs": [],
      "source": [
        "#Feature Selection Techniques\n",
        "# 1. Variance Threshold\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_var = selector.fit_transform(df.select_dtypes(include=np.number))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ui1Eet3TBj",
        "outputId": "68f5143b-4bf2-4a3a-a010-8895699c2488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            age  monthly_charges  total_charges  \\\n",
            "age                    1.000000         0.993399       0.907841   \n",
            "monthly_charges        0.993399         1.000000       0.949947   \n",
            "total_charges          0.907841         0.949947       1.000000   \n",
            "num_support_calls      0.960769         0.986241       0.988522   \n",
            "contract_length        0.866025         0.917663       0.995871   \n",
            "account_age_days       0.309054         0.416106       0.679358   \n",
            "days_since_last_login  0.767988         0.836385       0.965767   \n",
            "signup_month           0.993399         1.000000       0.949947   \n",
            "signup_quarter         0.866025         0.917663       0.995871   \n",
            "signup_dayofweek      -0.960769        -0.986241      -0.988522   \n",
            "avg_monthly_spend      0.965567         0.989036       0.985668   \n",
            "calls_per_month        0.981981         0.953821       0.812240   \n",
            "log_total_charges      0.947897         0.978184       0.994124   \n",
            "boxcox_total           0.983690         0.997830       0.968458   \n",
            "yj_support_calls       0.998684         0.997975       0.928155   \n",
            "\n",
            "                       num_support_calls  contract_length  account_age_days  \\\n",
            "age                             0.960769         0.866025          0.309054   \n",
            "monthly_charges                 0.986241         0.917663          0.416106   \n",
            "total_charges                   0.988522         0.995871          0.679358   \n",
            "num_support_calls               1.000000         0.970725          0.560702   \n",
            "contract_length                 0.970725         1.000000          0.743171   \n",
            "account_age_days                0.560702         0.743171          1.000000   \n",
            "days_since_last_login           0.915492         0.985329          0.846460   \n",
            "signup_month                    0.986241         0.917663          0.416106   \n",
            "signup_quarter                  0.970725         1.000000          0.743171   \n",
            "signup_dayofweek               -1.000000        -0.970725         -0.560702   \n",
            "avg_monthly_spend               0.999841         0.966282          0.545829   \n",
            "calls_per_month                 0.891042         0.755929          0.123754   \n",
            "log_total_charges               0.999067         0.980191          0.595932   \n",
            "boxcox_total                    0.994987         0.941837          0.475081   \n",
            "yj_support_calls                0.973731         0.890532          0.357430   \n",
            "\n",
            "                       days_since_last_login  signup_month  signup_quarter  \\\n",
            "age                                 0.767988      0.993399        0.866025   \n",
            "monthly_charges                     0.836385      1.000000        0.917663   \n",
            "total_charges                       0.965767      0.949947        0.995871   \n",
            "num_support_calls                   0.915492      0.986241        0.970725   \n",
            "contract_length                     0.985329      0.917663        1.000000   \n",
            "account_age_days                    0.846460      0.416106        0.743171   \n",
            "days_since_last_login               1.000000      0.836385        0.985329   \n",
            "signup_month                        0.836385      1.000000        0.917663   \n",
            "signup_quarter                      0.985329      0.917663        1.000000   \n",
            "signup_dayofweek                   -0.915492     -0.986241       -0.970725   \n",
            "avg_monthly_spend                   0.908163      0.989036        0.966282   \n",
            "calls_per_month                     0.633113      0.953821        0.755929   \n",
            "log_total_charges                   0.932011      0.978184        0.980191   \n",
            "boxcox_total                        0.870665      0.997830        0.941837   \n",
            "yj_support_calls                    0.799829      0.997975        0.890532   \n",
            "\n",
            "                       signup_dayofweek  avg_monthly_spend  calls_per_month  \\\n",
            "age                           -0.960769           0.965567         0.981981   \n",
            "monthly_charges               -0.986241           0.989036         0.953821   \n",
            "total_charges                 -0.988522           0.985668         0.812240   \n",
            "num_support_calls             -1.000000           0.999841         0.891042   \n",
            "contract_length               -0.970725           0.966282         0.755929   \n",
            "account_age_days              -0.560702           0.545829         0.123754   \n",
            "days_since_last_login         -0.915492           0.908163         0.633113   \n",
            "signup_month                  -0.986241           0.989036         0.953821   \n",
            "signup_quarter                -0.970725           0.966282         0.755929   \n",
            "signup_dayofweek               1.000000          -0.999841        -0.891042   \n",
            "avg_monthly_spend             -0.999841           1.000000         0.899004   \n",
            "calls_per_month               -0.891042           0.899004         1.000000   \n",
            "log_total_charges             -0.999067           0.998137         0.870611   \n",
            "boxcox_total                  -0.994987           0.996613         0.931971   \n",
            "yj_support_calls              -0.973731           0.977641         0.970994   \n",
            "\n",
            "                       log_total_charges  boxcox_total  yj_support_calls  \n",
            "age                             0.947897      0.983690          0.998684  \n",
            "monthly_charges                 0.978184      0.997830          0.997975  \n",
            "total_charges                   0.994124      0.968458          0.928155  \n",
            "num_support_calls               0.999067      0.994987          0.973731  \n",
            "contract_length                 0.980191      0.941837          0.890532  \n",
            "account_age_days                0.595932      0.475081          0.357430  \n",
            "days_since_last_login           0.932011      0.870665          0.799829  \n",
            "signup_month                    0.978184      0.997830          0.997975  \n",
            "signup_quarter                  0.980191      0.941837          0.890532  \n",
            "signup_dayofweek               -0.999067     -0.994987         -0.973731  \n",
            "avg_monthly_spend               0.998137      0.996613          0.977641  \n",
            "calls_per_month                 0.870611      0.931971          0.970994  \n",
            "log_total_charges               1.000000      0.989740          0.962990  \n",
            "boxcox_total                    0.989740      1.000000          0.991621  \n",
            "yj_support_calls                0.962990      0.991621          1.000000  \n"
          ]
        }
      ],
      "source": [
        "# 2. Correlation-Based Selection\n",
        "# Select only numeric columns for correlation calculation\n",
        "corr_matrix = df.select_dtypes(include=np.number).corr()\n",
        "print(corr_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiNLwIdB3jiT",
        "outputId": "321cccfd-267a-45ad-ce11-45a7ee81dd7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Importances:\n",
            " age                      0.112903\n",
            "account_age_days         0.112903\n",
            "days_since_last_login    0.108871\n",
            "total_charges            0.092742\n",
            "avg_monthly_spend        0.076613\n",
            "log_total_charges        0.076613\n",
            "calls_per_month          0.072581\n",
            "monthly_charges          0.068548\n",
            "num_support_calls        0.064516\n",
            "boxcox_total             0.060484\n",
            "signup_dayofweek         0.056452\n",
            "contract_length          0.044355\n",
            "signup_month             0.028226\n",
            "yj_support_calls         0.020161\n",
            "signup_quarter           0.004032\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# 3. Tree-Based Feature Importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df['churn'] = (df['account_age_days'] > df['account_age_days'].mean()).astype(int)\n",
        "\n",
        "X = df.select_dtypes(include=np.number).drop(columns=['churn'])\n",
        "y = df['churn']\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "print(\"Feature Importances:\\n\", pd.Series(importances, index=X.columns).sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "P6XHLp2Y39g-"
      },
      "outputs": [],
      "source": [
        "#Memory Optimization and Efficient Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f71fef9",
        "outputId": "73aca6e3-29d5-4675-c90c-a4adc17f0b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial memory usage of DataFrame: 1416 bytes\n"
          ]
        }
      ],
      "source": [
        "initial_memory_usage = df.memory_usage(deep=True).sum()\n",
        "print(f\"Initial memory usage of DataFrame: {initial_memory_usage} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0980b7ab",
        "outputId": "c97a6316-8bb9-4303-dc3f-b4326c30f30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined optimize_dataframe_memory function.\n"
          ]
        }
      ],
      "source": [
        "def optimize_dataframe_memory(df):\n",
        "    optimized_df = df.copy()\n",
        "\n",
        "    # Downcast numeric types\n",
        "    for col in optimized_df.select_dtypes(include=['int', 'float']).columns:\n",
        "        if optimized_df[col].dtype == 'int':\n",
        "            # Attempt to downcast integers\n",
        "            optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='integer')\n",
        "        elif optimized_df[col].dtype == 'float':\n",
        "            # Attempt to downcast floats to float32\n",
        "            optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='float')\n",
        "\n",
        "    # Convert object columns to category\n",
        "    for col in optimized_df.select_dtypes(include=['object']).columns:\n",
        "        num_unique_values = len(optimized_df[col].unique())\n",
        "        num_total_values = len(optimized_df[col])\n",
        "        if num_unique_values / num_total_values < 0.5: # Heuristic: if unique values are less than 50% of total rows\n",
        "            optimized_df[col] = optimized_df[col].astype('category')\n",
        "\n",
        "\n",
        "    return optimized_df\n",
        "\n",
        "print(\"Defined optimize_dataframe_memory function.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3efbc19",
        "outputId": "25f11126-a00b-431d-f169-55b6b50d1a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized DataFrame created.\n"
          ]
        }
      ],
      "source": [
        "df_optimized = optimize_dataframe_memory(df)\n",
        "print(\"Optimized DataFrame created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eee186fd",
        "outputId": "85dbc23e-0159-48c0-c274-3c8b04f3cc27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized memory usage of DataFrame: 1194 bytes\n",
            "Percentage memory reduction: 15.68%\n"
          ]
        }
      ],
      "source": [
        "optimized_memory_usage = df_optimized.memory_usage(deep=True).sum()\n",
        "print(f\"Optimized memory usage of DataFrame: {optimized_memory_usage} bytes\")\n",
        "\n",
        "percentage_reduction = ((initial_memory_usage - optimized_memory_usage) / initial_memory_usage) * 100\n",
        "print(f\"Percentage memory reduction: {percentage_reduction:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73459f15",
        "outputId": "7f841522-891a-4592-a959-77f6c6a488ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic CSV file 'large_data.csv' with 100000 rows created successfully!\n",
            "First 5 rows of the generated DataFrame:\n",
            "   transaction_id  customer_id product_category  order_quantity  \\\n",
            "0               0         2315      Electronics             4.0   \n",
            "1               1         2089      Electronics             1.0   \n",
            "2               2         7990         Clothing             6.0   \n",
            "3               3         8115             Food             2.0   \n",
            "4               4         1990       Home Goods             5.0   \n",
            "\n",
            "   price_per_unit transaction_date store_location payment_method  \\\n",
            "0          193.53       2023-12-01        Chicago    Credit Card   \n",
            "1          473.60       2023-11-17       New York  Bank Transfer   \n",
            "2           63.85       2023-05-07    Los Angeles         PayPal   \n",
            "3            9.52       2023-06-21    Los Angeles         PayPal   \n",
            "4           23.22       2023-05-02        Houston  Bank Transfer   \n",
            "\n",
            "   shipping_cost  \n",
            "0            NaN  \n",
            "1          14.52  \n",
            "2           2.84  \n",
            "3          22.22  \n",
            "4          11.08  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the number of rows for the synthetic dataset\n",
        "num_rows = 100000\n",
        "\n",
        "# Generate synthetic data\n",
        "data = {\n",
        "    \"transaction_id\": np.arange(num_rows),  # Integer\n",
        "    \"customer_id\": np.random.randint(1, 10000, size=num_rows),  # Integer, potentially lower cardinality for category\n",
        "    \"product_category\": np.random.choice([\"Electronics\", \"Clothing\", \"Home Goods\", \"Food\", \"Books\"], size=num_rows), # String (low cardinality)\n",
        "    \"order_quantity\": np.random.randint(1, 20, size=num_rows),  # Integer\n",
        "    \"price_per_unit\": np.round(np.random.uniform(5.0, 500.0, size=num_rows), 2),  # Float\n",
        "    \"transaction_date\": pd.to_datetime('2023-01-01') + pd.to_timedelta(np.random.randint(0, 365, size=num_rows), unit='D'), # Date\n",
        "    \"store_location\": np.random.choice([\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"], size=num_rows), # String (low cardinality)\n",
        "    \"payment_method\": np.random.choice([\"Credit Card\", \"PayPal\", \"Bank Transfer\"], size=num_rows), # String (low cardinality)\n",
        "    \"shipping_cost\": np.round(np.random.uniform(0.0, 25.0, size=num_rows), 2) # Float\n",
        "}\n",
        "\n",
        "df_large = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some missing values randomly for variety\n",
        "for col in ['order_quantity', 'price_per_unit', 'shipping_cost']:\n",
        "    df_large.loc[df_large.sample(frac=0.05).index, col] = np.nan\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file_path = \"large_data.csv\"\n",
        "df_large.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"Synthetic CSV file '{csv_file_path}' with {num_rows} rows created successfully!\")\n",
        "print(\"First 5 rows of the generated DataFrame:\")\n",
        "print(df_large.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98f30f5d",
        "outputId": "631d2b94-fa4f-4acd-e032-cd441959bc23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmark - Loading Time: 0.1446 seconds\n",
            "Benchmark - Memory Usage: 27159210 bytes\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "csv_file_path = \"large_data.csv\"\n",
        "\n",
        "# Measure time to load without optimizations\n",
        "start_time = time.time()\n",
        "df_benchmark = pd.read_csv(csv_file_path)\n",
        "end_time = time.time()\n",
        "\n",
        "benchmark_loading_time = end_time - start_time\n",
        "benchmark_memory_usage = df_benchmark.memory_usage(deep=True).sum()\n",
        "\n",
        "print(f\"Benchmark - Loading Time: {benchmark_loading_time:.4f} seconds\")\n",
        "print(f\"Benchmark - Memory Usage: {benchmark_memory_usage} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02e00fcc",
        "outputId": "90e73d34-0522-4664-f9f8-460113e36c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selective Columns Loading - Loading Time: 0.0831 seconds\n",
            "Selective Columns Loading - Memory Usage: 8859664 bytes\n",
            "First 5 rows of selectively loaded DataFrame:\n",
            "   transaction_id  customer_id product_category  order_quantity  shipping_cost\n",
            "0               0         2315      Electronics             4.0            NaN\n",
            "1               1         2089      Electronics             1.0          14.52\n",
            "2               2         7990         Clothing             6.0           2.84\n",
            "3               3         8115             Food             2.0          22.22\n",
            "4               4         1990       Home Goods             5.0          11.08\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Identify a subset of columns to load\n",
        "selected_columns = [\"transaction_id\", \"customer_id\", \"product_category\", \"order_quantity\", \"shipping_cost\"]\n",
        "\n",
        "# Measure time to load with selective columns\n",
        "start_time = time.time()\n",
        "df_selected_cols = pd.read_csv(csv_file_path, usecols=selected_columns)\n",
        "end_time = time.time()\n",
        "\n",
        "selected_cols_loading_time = end_time - start_time\n",
        "selected_cols_memory_usage = df_selected_cols.memory_usage(deep=True).sum()\n",
        "\n",
        "print(f\"Selective Columns Loading - Loading Time: {selected_cols_loading_time:.4f} seconds\")\n",
        "print(f\"Selective Columns Loading - Memory Usage: {selected_cols_memory_usage} bytes\")\n",
        "print(\"First 5 rows of selectively loaded DataFrame:\")\n",
        "print(df_selected_cols.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed0fb38f",
        "outputId": "139aa6e3-0188-4db8-8fc2-820f21296cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized Dtypes Loading - Loading Time: 0.8134 seconds\n",
            "Optimized Dtypes Loading - Memory Usage: 2901329 bytes\n",
            "First 5 rows of DataFrame with optimized dtypes:\n",
            "   transaction_id  customer_id product_category  order_quantity  \\\n",
            "0               0         2315      Electronics             4.0   \n",
            "1               1         2089      Electronics             1.0   \n",
            "2               2         7990         Clothing             6.0   \n",
            "3               3         8115             Food             2.0   \n",
            "4               4         1990       Home Goods             5.0   \n",
            "\n",
            "   price_per_unit transaction_date store_location payment_method  \\\n",
            "0      193.529999       2023-12-01        Chicago    Credit Card   \n",
            "1      473.600006       2023-11-17       New York  Bank Transfer   \n",
            "2       63.849998       2023-05-07    Los Angeles         PayPal   \n",
            "3        9.520000       2023-06-21    Los Angeles         PayPal   \n",
            "4       23.219999       2023-05-02        Houston  Bank Transfer   \n",
            "\n",
            "   shipping_cost  \n",
            "0            NaN  \n",
            "1      14.520000  \n",
            "2       2.840000  \n",
            "3      22.219999  \n",
            "4      11.080000  \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Define efficient dtypes for columns\n",
        "dtype_optimizations = {\n",
        "    \"transaction_id\": \"int32\",\n",
        "    \"customer_id\": \"int16\",\n",
        "    \"product_category\": \"category\",\n",
        "    \"order_quantity\": \"float32\",  # Using float32 due to potential NaN values from random introduction\n",
        "    \"price_per_unit\": \"float32\",  # Using float32 due to potential NaN values\n",
        "    \"store_location\": \"category\",\n",
        "    \"payment_method\": \"category\",\n",
        "    \"shipping_cost\": \"float32\"  # Using float32 due to potential NaN values\n",
        "}\n",
        "\n",
        "# Columns that need to be parsed as dates\n",
        "parse_dates_cols = [\"transaction_date\"]\n",
        "\n",
        "# Measure time to load with efficient dtypes\n",
        "start_time = time.time()\n",
        "df_optimized_dtypes = pd.read_csv(\n",
        "    csv_file_path,\n",
        "    dtype=dtype_optimizations,\n",
        "    parse_dates=parse_dates_cols\n",
        ")\n",
        "end_time = time.time()\n",
        "\n",
        "optimized_dtypes_loading_time = end_time - start_time\n",
        "optimized_dtypes_memory_usage = df_optimized_dtypes.memory_usage(deep=True).sum()\n",
        "\n",
        "print(f\"Optimized Dtypes Loading - Loading Time: {optimized_dtypes_loading_time:.4f} seconds\")\n",
        "print(f\"Optimized Dtypes Loading - Memory Usage: {optimized_dtypes_memory_usage} bytes\")\n",
        "print(\"First 5 rows of DataFrame with optimized dtypes:\")\n",
        "print(df_optimized_dtypes.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f2a352f",
        "outputId": "c3609bd1-4171-4510-fcb0-a5fb58788c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed chunk 1, sum of order_quantity: 96179.0\n",
            "Processed chunk 2, sum of order_quantity: 94368.0\n",
            "Processed chunk 3, sum of order_quantity: 95796.0\n",
            "\n",
            "Chunked Processing - Total Time: 0.1969 seconds\n",
            "Aggregated results from chunks (first 5): [np.float64(96179.0), np.float64(94368.0), np.float64(95796.0), np.float64(94634.0), np.float64(94787.0)]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "chunk_size = 10000  # Define a chunk size\n",
        "\n",
        "# Measure time and process data in chunks\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize an empty list to store processed chunks or aggregate results\n",
        "processed_chunks = []\n",
        "\n",
        "for i, chunk_df in enumerate(pd.read_csv(csv_file_path, chunksize=chunk_size)):\n",
        "    # Example processing: calculate sum of 'order_quantity' for each chunk\n",
        "    # In a real scenario, you would perform desired operations on each chunk\n",
        "    # without loading the entire dataset into memory.\n",
        "    chunk_sum_quantity = chunk_df['order_quantity'].sum()\n",
        "    processed_chunks.append(chunk_sum_quantity)\n",
        "    if i < 3: # Print for first few chunks to show progress/example\n",
        "        print(f\"Processed chunk {i+1}, sum of order_quantity: {chunk_sum_quantity}\")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "chunked_loading_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nChunked Processing - Total Time: {chunked_loading_time:.4f} seconds\")\n",
        "print(f\"Aggregated results from chunks (first 5): {processed_chunks[:5]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FmVkGdNZ5rJK"
      },
      "outputs": [],
      "source": [
        "#Data Pipelines and Practical Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cea2f7e",
        "outputId": "96b4c638-ac48-4cac-84c9-8b9ef23ccc49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70d7ea7b",
        "outputId": "573fac62-b43c-41c4-f40c-9767e109a5bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataCleaningPipeline class defined.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders import BinaryEncoder, TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.imputers = {}\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.logger.info(\"DataCleaningPipeline initialized.\")\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        logger = logging.getLogger(self.__class__.__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "print(\"DataCleaningPipeline class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41c0ac2",
        "outputId": "b37cb710-77e5-41bf-c163-8fdab37d625e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "handle_missing_values method added to DataCleaningPipeline class.\n"
          ]
        }
      ],
      "source": [
        "#Implement handle_missing_values Method\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders import BinaryEncoder, TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.imputers = {}\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.logger.info(\"DataCleaningPipeline initialized.\")\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        logger = logging.getLogger(self.__class__.__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "    def handle_missing_values(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        missing_values_config = self.config.get('missing_values_config', [])\n",
        "        self.logger.info(f\"Starting missing value imputation in '{mode}' mode.\")\n",
        "\n",
        "        for item in missing_values_config:\n",
        "            column = item.get('column')\n",
        "            strategy = item.get('imputation_strategy')\n",
        "            fill_value = item.get('fill_value', None) # Only for 'constant' strategy\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping imputation for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if strategy in ['mean', 'median', 'mode', 'constant']:\n",
        "                    imputer_key = f\"{column}_{strategy}\"\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if imputer_key not in self.imputers:\n",
        "                            if strategy == 'constant':\n",
        "                                if fill_value is None:\n",
        "                                    self.logger.error(f\"'fill_value' must be provided for 'constant' strategy in column '{column}'. Skipping.\")\n",
        "                                    continue\n",
        "                                imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
        "                            else:\n",
        "                                imputer = SimpleImputer(strategy=strategy)\n",
        "\n",
        "                            # Fit on the non-NaN values for mean/median/mode, or on all for constant\n",
        "                            imputer.fit(df[[column]])\n",
        "                            self.imputers[imputer_key] = imputer\n",
        "                            self.logger.info(f\"Fitted {strategy} imputer for column '{column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if imputer_key in self.imputers:\n",
        "                            df[column] = self.imputers[imputer_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied {strategy} imputation to column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted imputer found for column '{column}' with strategy '{strategy}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif strategy in ['ffill', 'bfill']:\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if strategy == 'ffill':\n",
        "                            df[column] = df[column].ffill()\n",
        "                            self.logger.info(f\"Applied forward-fill imputation to column '{column}'.\")\n",
        "                        elif strategy == 'bfill':\n",
        "                            df[column] = df[column].bfill()\n",
        "                            self.logger.info(f\"Applied backward-fill imputation to column '{column}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported imputation strategy '{strategy}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during imputation for column '{column}' with strategy '{strategy}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished missing value imputation.\")\n",
        "        return df\n",
        "\n",
        "print(\"handle_missing_values method added to DataCleaningPipeline class.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39c3556",
        "outputId": "2c5647be-7208-494c-cec1-7d5703f7d99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "handle_outliers method added to DataCleaningPipeline class.\n"
          ]
        }
      ],
      "source": [
        "#Implement handle_outliers Method\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders import BinaryEncoder, TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.imputers = {}\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.logger.info(\"DataCleaningPipeline initialized.\")\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        logger = logging.getLogger(self.__class__.__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "    def handle_missing_values(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        missing_values_config = self.config.get('missing_values_config', [])\n",
        "        self.logger.info(f\"Starting missing value imputation in '{mode}' mode.\")\n",
        "\n",
        "        for item in missing_values_config:\n",
        "            column = item.get('column')\n",
        "            strategy = item.get('imputation_strategy')\n",
        "            fill_value = item.get('fill_value', None)\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping imputation for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if strategy in ['mean', 'median', 'mode', 'constant']:\n",
        "                    imputer_key = f\"{column}_{strategy}\"\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if imputer_key not in self.imputers:\n",
        "                            if strategy == 'constant':\n",
        "                                if fill_value is None:\n",
        "                                    self.logger.error(f\"'fill_value' must be provided for 'constant' strategy in column '{column}'. Skipping.\")\n",
        "                                    continue\n",
        "                                imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
        "                            else:\n",
        "                                imputer = SimpleImputer(strategy=strategy)\n",
        "\n",
        "                            imputer.fit(df[[column]])\n",
        "                            self.imputers[imputer_key] = imputer\n",
        "                            self.logger.info(f\"Fitted {strategy} imputer for column '{column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if imputer_key in self.imputers:\n",
        "                            df[column] = self.imputers[imputer_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied {strategy} imputation to column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted imputer found for column '{column}' with strategy '{strategy}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif strategy in ['ffill', 'bfill']:\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if strategy == 'ffill':\n",
        "                            df[column] = df[column].ffill()\n",
        "                            self.logger.info(f\"Applied forward-fill imputation to column '{column}'.\")\n",
        "                        elif strategy == 'bfill':\n",
        "                            df[column] = df[column].bfill()\n",
        "                            self.logger.info(f\"Applied backward-fill imputation to column '{column}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported imputation strategy '{strategy}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during imputation for column '{column}' with strategy '{strategy}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished missing value imputation.\")\n",
        "        return df\n",
        "\n",
        "    def handle_outliers(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        outlier_config = self.config.get('outlier_config', [])\n",
        "        self.logger.info(f\"Starting outlier handling in '{mode}' mode.\")\n",
        "\n",
        "        for item in outlier_config:\n",
        "            column = item.get('column')\n",
        "            detection_method = item.get('detection_method')\n",
        "            treatment_method = item.get('treatment_method')\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if treatment_method != 'capping':\n",
        "                self.logger.warning(f\"Unsupported treatment method '{treatment_method}' for column '{column}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            lower_bound_key = f'{column}_{detection_method}_lower_bound'\n",
        "            upper_bound_key = f'{column}_{detection_method}_upper_bound'\n",
        "\n",
        "            try:\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if detection_method == 'iqr':\n",
        "                        iqr_multiplier = item.get('iqr_multiplier', 1.5)\n",
        "                        Q1 = df[column].quantile(0.25)\n",
        "                        Q3 = df[column].quantile(0.75)\n",
        "                        IQR = Q3 - Q1\n",
        "                        lower_bound = Q1 - iqr_multiplier * IQR\n",
        "                        upper_bound = Q3 + iqr_multiplier * IQR\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted IQR bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    elif detection_method == 'zscore':\n",
        "                        zscore_threshold = item.get('zscore_threshold', 3)\n",
        "                        mean = df[column].mean()\n",
        "                        std = df[column].std()\n",
        "                        lower_bound = mean - zscore_threshold * std\n",
        "                        upper_bound = mean + zscore_threshold * std\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted Z-score bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Unsupported detection method '{detection_method}' for column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if lower_bound_key in self.scalers and upper_bound_key in self.scalers:\n",
        "                        lower_bound = self.scalers[lower_bound_key]\n",
        "                        upper_bound = self.scalers[upper_bound_key]\n",
        "\n",
        "                        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
        "                        self.logger.info(f\"Applied capping to column '{column}' using {detection_method} bounds. Values clipped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Bounds for column '{column}' with detection method '{detection_method}' not found in scalers. Skipping transform.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during outlier handling for column '{column}' with method '{detection_method}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished outlier handling.\")\n",
        "        return df\n",
        "\n",
        "print(\"handle_outliers method added to DataCleaningPipeline class.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc8a27b9",
        "outputId": "5c3e8b31-49c2-4bdb-a0ee-8cd4c8303a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encode_categorical method added to DataCleaningPipeline class.\n"
          ]
        }
      ],
      "source": [
        "#Implement encode_categorical Method\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders import BinaryEncoder, TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.imputers = {}\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.logger.info(\"DataCleaningPipeline initialized.\")\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        logger = logging.getLogger(self.__class__.__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "    def handle_missing_values(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        missing_values_config = self.config.get('missing_values_config', [])\n",
        "        self.logger.info(f\"Starting missing value imputation in '{mode}' mode.\")\n",
        "\n",
        "        for item in missing_values_config:\n",
        "            column = item.get('column')\n",
        "            strategy = item.get('imputation_strategy')\n",
        "            fill_value = item.get('fill_value', None)\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping imputation for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if strategy in ['mean', 'median', 'mode', 'constant']:\n",
        "                    imputer_key = f\"{column}_{strategy}\"\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if imputer_key not in self.imputers:\n",
        "                            if strategy == 'constant':\n",
        "                                if fill_value is None:\n",
        "                                    self.logger.error(f\"'fill_value' must be provided for 'constant' strategy in column '{column}'. Skipping.\")\n",
        "                                    continue\n",
        "                                imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
        "                            else:\n",
        "                                imputer = SimpleImputer(strategy=strategy)\n",
        "\n",
        "                            imputer.fit(df[[column]])\n",
        "                            self.imputers[imputer_key] = imputer\n",
        "                            self.logger.info(f\"Fitted {strategy} imputer for column '{column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if imputer_key in self.imputers:\n",
        "                            df[column] = self.imputers[imputer_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied {strategy} imputation to column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted imputer found for column '{column}' with strategy '{strategy}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif strategy in ['ffill', 'bfill']:\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if strategy == 'ffill':\n",
        "                            df[column] = df[column].ffill()\n",
        "                            self.logger.info(f\"Applied forward-fill imputation to column '{column}'.\")\n",
        "                        elif strategy == 'bfill':\n",
        "                            df[column] = df[column].bfill()\n",
        "                            self.logger.info(f\"Applied backward-fill imputation to column '{column}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported imputation strategy '{strategy}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during imputation for column '{column}' with strategy '{strategy}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished missing value imputation.\")\n",
        "        return df\n",
        "\n",
        "    def handle_outliers(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        outlier_config = self.config.get('outlier_config', [])\n",
        "        self.logger.info(f\"Starting outlier handling in '{mode}' mode.\")\n",
        "\n",
        "        for item in outlier_config:\n",
        "            column = item.get('column')\n",
        "            detection_method = item.get('detection_method')\n",
        "            treatment_method = item.get('treatment_method')\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if treatment_method != 'capping':\n",
        "                self.logger.warning(f\"Unsupported treatment method '{treatment_method}' for column '{column}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            lower_bound_key = f'{column}_{detection_method}_lower_bound'\n",
        "            upper_bound_key = f'{column}_{detection_method}_upper_bound'\n",
        "\n",
        "            try:\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if detection_method == 'iqr':\n",
        "                        iqr_multiplier = item.get('iqr_multiplier', 1.5)\n",
        "                        Q1 = df[column].quantile(0.25)\n",
        "                        Q3 = df[column].quantile(0.75)\n",
        "                        IQR = Q3 - Q1\n",
        "                        lower_bound = Q1 - iqr_multiplier * IQR\n",
        "                        upper_bound = Q3 + iqr_multiplier * IQR\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted IQR bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    elif detection_method == 'zscore':\n",
        "                        zscore_threshold = item.get('zscore_threshold', 3)\n",
        "                        mean = df[column].mean()\n",
        "                        std = df[column].std()\n",
        "                        lower_bound = mean - zscore_threshold * std\n",
        "                        upper_bound = mean + zscore_threshold * std\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted Z-score bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Unsupported detection method '{detection_method}' for column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if lower_bound_key in self.scalers and upper_bound_key in self.scalers:\n",
        "                        lower_bound = self.scalers[lower_bound_key]\n",
        "                        upper_bound = self.scalers[upper_bound_key]\n",
        "\n",
        "                        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
        "                        self.logger.info(f\"Applied capping to column '{column}' using {detection_method} bounds. Values clipped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Bounds for column '{column}' with detection method '{detection_method}' not found in scalers. Skipping transform.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during outlier handling for column '{column}' with method '{detection_method}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished outlier handling.\")\n",
        "        return df\n",
        "\n",
        "    def encode_categorical(self, X, y=None, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        categorical_encoding_config = self.config.get('categorical_encoding_config', [])\n",
        "        self.logger.info(f\"Starting categorical encoding in '{mode}' mode.\")\n",
        "\n",
        "        for item in categorical_encoding_config:\n",
        "            column = item.get('column')\n",
        "            encoding_type = item.get('encoding_type')\n",
        "            replace_original = item.get('replace_original', True) # Default to replacing original\n",
        "            params = item.get('params', {}) # For additional encoder parameters\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            # Check if the column is categorical or object type\n",
        "            if not pd.api.types.is_categorical_dtype(df[column]) and not pd.api.types.is_object_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not categorical or object type. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                encoder_key = f\"{column}_{encoding_type}\"\n",
        "\n",
        "                if encoding_type == 'label':\n",
        "                    mapping = params.get('mapping')\n",
        "                    if mapping: # Use explicit mapping if provided\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            new_column_name = f'{column}_encoded'\n",
        "                            df[new_column_name] = df[column].map(mapping)\n",
        "                            self.logger.info(f\"Applied label encoding (mapping) to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                    else: # Fallback to sklearn LabelEncoder if no explicit mapping\n",
        "                        self.logger.warning(f\"No explicit mapping provided for label encoding of column '{column}'. Using sklearn LabelEncoder.\")\n",
        "                        if mode in ['fit', 'fit_transform']:\n",
        "                            if encoder_key not in self.encoders:\n",
        "                                le = LabelEncoder()\n",
        "                                le.fit(df[column].astype(str)) # Convert to string to handle potential NaN/non-string types\n",
        "                                self.encoders[encoder_key] = le\n",
        "                                self.logger.info(f\"Fitted LabelEncoder for column '{column}'.\")\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            if encoder_key in self.encoders:\n",
        "                                new_column_name = f'{column}_encoded'\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[column].astype(str))\n",
        "                                self.logger.info(f\"Applied LabelEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                                if replace_original:\n",
        "                                    df = df.drop(columns=[column])\n",
        "                                    self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                            else:\n",
        "                                self.logger.warning(f\"No fitted LabelEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'one_hot':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, **params)\n",
        "                            ohe.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = ohe\n",
        "                            self.logger.info(f\"Fitted OneHotEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            ohe_cols = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            feature_names = self.encoders[encoder_key].get_feature_names_out([column])\n",
        "                            ohe_df = pd.DataFrame(ohe_cols, index=df.index, columns=feature_names)\n",
        "                            df = pd.concat([df, ohe_df], axis=1)\n",
        "                            self.logger.info(f\"Applied OneHotEncoder to column '{column}'. Added {len(feature_names)} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted OneHotEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'frequency':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            # Normalize=True to get proportions/frequencies\n",
        "                            freq_map = df[column].value_counts(normalize=True).to_dict()\n",
        "                            self.encoders[encoder_key] = freq_map\n",
        "                            self.logger.info(f\"Fitted frequency map for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_freq_encoded'\n",
        "                            df[new_column_name] = df[column].map(self.encoders[encoder_key])\n",
        "                            self.logger.info(f\"Applied frequency encoding to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted frequency map found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'binary':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            be = BinaryEncoder(cols=[column], handle_unknown='ignore', **params)\n",
        "                            be.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = be\n",
        "                            self.logger.info(f\"Fitted BinaryEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            binary_cols_df = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            df = pd.concat([df, binary_cols_df], axis=1)\n",
        "                            self.logger.info(f\"Applied BinaryEncoder to column '{column}'. Added {binary_cols_df.shape[1]} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted BinaryEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'target':\n",
        "                    target_column = item.get('target_column')\n",
        "                    if target_column is None:\n",
        "                        self.logger.error(f\"'target_column' must be specified for target encoding of column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if y is None:\n",
        "                            self.logger.error(f\"'y' (target variable) must be provided for target encoding in fit/fit_transform mode for column '{column}'. Skipping.\")\n",
        "                            continue\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            te = TargetEncoder(cols=[column], handle_unknown='value', **params)\n",
        "                            # Ensure y is a Series for fitting\n",
        "                            target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                            te.fit(df[[column]], target_series)\n",
        "                            self.encoders[encoder_key] = te\n",
        "                            self.logger.info(f\"Fitted TargetEncoder for column '{column}' with target '{target_column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_target_encoded'\n",
        "                            # For transform, y is optional, if provided for smoothing, use it.\n",
        "                            # If y is provided for fit_transform, it should be passed here too.\n",
        "                            if y is not None:\n",
        "                                target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]], target_series)\n",
        "                            else:\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied TargetEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted TargetEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported encoding type '{encoding_type}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during {encoding_type} encoding for column '{column}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished categorical encoding.\")\n",
        "        return df\n",
        "\n",
        "print(\"encode_categorical method added to DataCleaningPipeline class.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5564961e",
        "outputId": "8f19ad3e-9d80-46eb-f0f9-3a14a242e177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scale_numerical_features method added to DataCleaningPipeline class.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders import BinaryEncoder, TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.imputers = {}\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.logger.info(\"DataCleaningPipeline initialized.\")\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        logger = logging.getLogger(self.__class__.__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "    def handle_missing_values(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        missing_values_config = self.config.get('missing_values_config', [])\n",
        "        self.logger.info(f\"Starting missing value imputation in '{mode}' mode.\")\n",
        "\n",
        "        for item in missing_values_config:\n",
        "            column = item.get('column')\n",
        "            strategy = item.get('imputation_strategy')\n",
        "            fill_value = item.get('fill_value', None)\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping imputation for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if strategy in ['mean', 'median', 'mode', 'constant']:\n",
        "                    imputer_key = f\"{column}_{strategy}\"\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if imputer_key not in self.imputers:\n",
        "                            if strategy == 'constant':\n",
        "                                if fill_value is None:\n",
        "                                    self.logger.error(f\"'fill_value' must be provided for 'constant' strategy in column '{column}'. Skipping.\")\n",
        "                                    continue\n",
        "                                imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
        "                            else:\n",
        "                                imputer = SimpleImputer(strategy=strategy)\n",
        "\n",
        "                            imputer.fit(df[[column]])\n",
        "                            self.imputers[imputer_key] = imputer\n",
        "                            self.logger.info(f\"Fitted {strategy} imputer for column '{column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if imputer_key in self.imputers:\n",
        "                            df[column] = self.imputers[imputer_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied {strategy} imputation to column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted imputer found for column '{column}' with strategy '{strategy}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif strategy in ['ffill', 'bfill']:\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if strategy == 'ffill':\n",
        "                            df[column] = df[column].ffill()\n",
        "                            self.logger.info(f\"Applied forward-fill imputation to column '{column}'.\")\n",
        "                        elif strategy == 'bfill':\n",
        "                            df[column] = df[column].bfill()\n",
        "                            self.logger.info(f\"Applied backward-fill imputation to column '{column}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported imputation strategy '{strategy}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during imputation for column '{column}' with strategy '{strategy}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished missing value imputation.\")\n",
        "        return df\n",
        "\n",
        "    def handle_outliers(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        outlier_config = self.config.get('outlier_config', [])\n",
        "        self.logger.info(f\"Starting outlier handling in '{mode}' mode.\")\n",
        "\n",
        "        for item in outlier_config:\n",
        "            column = item.get('column')\n",
        "            detection_method = item.get('detection_method')\n",
        "            treatment_method = item.get('treatment_method')\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if treatment_method != 'capping':\n",
        "                self.logger.warning(f\"Unsupported treatment method '{treatment_method}' for column '{column}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            lower_bound_key = f'{column}_{detection_method}_lower_bound'\n",
        "            upper_bound_key = f'{column}_{detection_method}_upper_bound'\n",
        "\n",
        "            try:\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if detection_method == 'iqr':\n",
        "                        iqr_multiplier = item.get('iqr_multiplier', 1.5)\n",
        "                        Q1 = df[column].quantile(0.25)\n",
        "                        Q3 = df[column].quantile(0.75)\n",
        "                        IQR = Q3 - Q1\n",
        "                        lower_bound = Q1 - iqr_multiplier * IQR\n",
        "                        upper_bound = Q3 + iqr_multiplier * IQR\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted IQR bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    elif detection_method == 'zscore':\n",
        "                        zscore_threshold = item.get('zscore_threshold', 3)\n",
        "                        mean = df[column].mean()\n",
        "                        std = df[column].std()\n",
        "                        lower_bound = mean - zscore_threshold * std\n",
        "                        upper_bound = mean + zscore_threshold * std\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted Z-score bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Unsupported detection method '{detection_method}' for column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if lower_bound_key in self.scalers and upper_bound_key in self.scalers:\n",
        "                        lower_bound = self.scalers[lower_bound_key]\n",
        "                        upper_bound = self.scalers[upper_bound_key]\n",
        "\n",
        "                        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
        "                        self.logger.info(f\"Applied capping to column '{column}' using {detection_method} bounds. Values clipped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Bounds for column '{column}' with detection method '{detection_method}' not found in scalers. Skipping transform.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during outlier handling for column '{column}' with method '{detection_method}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished outlier handling.\")\n",
        "        return df\n",
        "\n",
        "    def encode_categorical(self, X, y=None, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        categorical_encoding_config = self.config.get('categorical_encoding_config', [])\n",
        "        self.logger.info(f\"Starting categorical encoding in '{mode}' mode.\")\n",
        "\n",
        "        for item in categorical_encoding_config:\n",
        "            column = item.get('column')\n",
        "            encoding_type = item.get('encoding_type')\n",
        "            replace_original = item.get('replace_original', True) # Default to replacing original\n",
        "            params = item.get('params', {}) # For additional encoder parameters\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            # Check if the column is categorical or object type\n",
        "            if not pd.api.types.is_categorical_dtype(df[column]) and not pd.api.types.is_object_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not categorical or object type. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                encoder_key = f\"{column}_{encoding_type}\"\n",
        "\n",
        "                if encoding_type == 'label':\n",
        "                    mapping = params.get('mapping')\n",
        "                    if mapping: # Use explicit mapping if provided\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            new_column_name = f'{column}_encoded'\n",
        "                            df[new_column_name] = df[column].map(mapping)\n",
        "                            self.logger.info(f\"Applied label encoding (mapping) to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                    else: # Fallback to sklearn LabelEncoder if no explicit mapping\n",
        "                        self.logger.warning(f\"No explicit mapping provided for label encoding of column '{column}'. Using sklearn LabelEncoder.\")\n",
        "                        if mode in ['fit', 'fit_transform']:\n",
        "                            if encoder_key not in self.encoders:\n",
        "                                le = LabelEncoder()\n",
        "                                le.fit(df[column].astype(str)) # Convert to string to handle potential NaN/non-string types\n",
        "                                self.encoders[encoder_key] = le\n",
        "                                self.logger.info(f\"Fitted LabelEncoder for column '{column}'.\")\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            if encoder_key in self.encoders:\n",
        "                                new_column_name = f'{column}_encoded'\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[column].astype(str))\n",
        "                                self.logger.info(f\"Applied LabelEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                                if replace_original:\n",
        "                                    df = df.drop(columns=[column])\n",
        "                                    self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                            else:\n",
        "                                self.logger.warning(f\"No fitted LabelEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'one_hot':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, **params)\n",
        "                            ohe.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = ohe\n",
        "                            self.logger.info(f\"Fitted OneHotEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            ohe_cols = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            feature_names = self.encoders[encoder_key].get_feature_names_out([column])\n",
        "                            ohe_df = pd.DataFrame(ohe_cols, index=df.index, columns=feature_names)\n",
        "                            df = pd.concat([df, ohe_df], axis=1)\n",
        "                            self.logger.info(f\"Applied OneHotEncoder to column '{column}'. Added {len(feature_names)} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted OneHotEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'frequency':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            # Normalize=True to get proportions/frequencies\n",
        "                            freq_map = df[column].value_counts(normalize=True).to_dict()\n",
        "                            self.encoders[encoder_key] = freq_map\n",
        "                            self.logger.info(f\"Fitted frequency map for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_freq_encoded'\n",
        "                            df[new_column_name] = df[column].map(self.encoders[encoder_key])\n",
        "                            self.logger.info(f\"Applied frequency encoding to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted frequency map found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'binary':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            be = BinaryEncoder(cols=[column], handle_unknown='ignore', **params)\n",
        "                            be.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = be\n",
        "                            self.logger.info(f\"Fitted BinaryEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            binary_cols_df = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            df = pd.concat([df, binary_cols_df], axis=1)\n",
        "                            self.logger.info(f\"Applied BinaryEncoder to column '{column}'. Added {binary_cols_df.shape[1]} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted BinaryEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'target':\n",
        "                    target_column = item.get('target_column')\n",
        "                    if target_column is None:\n",
        "                        self.logger.error(f\"'target_column' must be specified for target encoding of column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if y is None:\n",
        "                            self.logger.error(f\"'y' (target variable) must be provided for target encoding in fit/fit_transform mode for column '{column}'. Skipping.\")\n",
        "                            continue\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            te = TargetEncoder(cols=[column], handle_unknown='value', **params)\n",
        "                            # Ensure y is a Series for fitting\n",
        "                            target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                            te.fit(df[[column]], target_series)\n",
        "                            self.encoders[encoder_key] = te\n",
        "                            self.logger.info(f\"Fitted TargetEncoder for column '{column}' with target '{target_column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_target_encoded'\n",
        "                            # For transform, y is optional, if provided for smoothing, use it.\n",
        "                            # If y is provided for fit_transform, it should be passed here too.\n",
        "                            if y is not None:\n",
        "                                target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]], target_series)\n",
        "                            else:\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied TargetEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted TargetEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported encoding type '{encoding_type}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during {encoding_type} encoding for column '{column}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished categorical encoding.\")\n",
        "        return df\n",
        "\n",
        "    def scale_numerical_features(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        scaling_config = self.config.get('scaling_config', [])\n",
        "        self.logger.info(f\"Starting numerical feature scaling in '{mode}' mode.\")\n",
        "\n",
        "        for item in scaling_config:\n",
        "            column = item.get('column')\n",
        "            scaling_type = item.get('scaling_type')\n",
        "            params = item.get('params', {}) # For additional scaler parameters\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping scaling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping scaling for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                scaler_key = f\"{column}_{scaling_type}\"\n",
        "\n",
        "                if scaling_type == 'standard':\n",
        "                    scaler_class = StandardScaler\n",
        "                elif scaling_type == 'minmax':\n",
        "                    scaler_class = MinMaxScaler\n",
        "                elif scaling_type == 'robust':\n",
        "                    scaler_class = RobustScaler\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported scaling type '{scaling_type}' for column '{column}'. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if scaler_key not in self.scalers:\n",
        "                        scaler = scaler_class(**params)\n",
        "                        scaler.fit(df[[column]])\n",
        "                        self.scalers[scaler_key] = scaler\n",
        "                        self.logger.info(f\"Fitted {scaling_type} scaler for column '{column}'.\")\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if scaler_key in self.scalers:\n",
        "                        df[column] = self.scalers[scaler_key].transform(df[[column]])\n",
        "                        self.logger.info(f\"Applied {scaling_type} scaling to column '{column}'.\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"No fitted {scaling_type} scaler found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during {scaling_type} scaling for column '{column}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished numerical feature scaling.\")\n",
        "        return df\n",
        "\n",
        "print(\"scale_numerical_features method added to DataCleaningPipeline class.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d525fca3",
        "outputId": "b65f07cc-c714-4985-accd-85c321190a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "engineer_features method added to DataCleaningPipeline class.\n"
          ]
        }
      ],
      "source": [
        "#Implement engineer_features Method\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders import BinaryEncoder, TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.imputers = {}\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.logger.info(\"DataCleaningPipeline initialized.\")\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        logger = logging.getLogger(self.__class__.__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "    def handle_missing_values(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        missing_values_config = self.config.get('missing_values_config', [])\n",
        "        self.logger.info(f\"Starting missing value imputation in '{mode}' mode.\")\n",
        "\n",
        "        for item in missing_values_config:\n",
        "            column = item.get('column')\n",
        "            strategy = item.get('imputation_strategy')\n",
        "            fill_value = item.get('fill_value', None)\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping imputation for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if strategy in ['mean', 'median', 'mode', 'constant']:\n",
        "                    imputer_key = f\"{column}_{strategy}\"\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if imputer_key not in self.imputers:\n",
        "                            if strategy == 'constant':\n",
        "                                if fill_value is None:\n",
        "                                    self.logger.error(f\"'fill_value' must be provided for 'constant' strategy in column '{column}'. Skipping.\")\n",
        "                                    continue\n",
        "                                imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
        "                            else:\n",
        "                                imputer = SimpleImputer(strategy=strategy)\n",
        "\n",
        "                            imputer.fit(df[[column]])\n",
        "                            self.imputers[imputer_key] = imputer\n",
        "                            self.logger.info(f\"Fitted {strategy} imputer for column '{column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if imputer_key in self.imputers:\n",
        "                            df[column] = self.imputers[imputer_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied {strategy} imputation to column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted imputer found for column '{column}' with strategy '{strategy}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif strategy in ['ffill', 'bfill']:\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if strategy == 'ffill':\n",
        "                            df[column] = df[column].ffill()\n",
        "                            self.logger.info(f\"Applied forward-fill imputation to column '{column}'.\")\n",
        "                        elif strategy == 'bfill':\n",
        "                            df[column] = df[column].bfill()\n",
        "                            self.logger.info(f\"Applied backward-fill imputation to column '{column}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported imputation strategy '{strategy}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during imputation for column '{column}' with strategy '{strategy}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished missing value imputation.\")\n",
        "        return df\n",
        "\n",
        "    def handle_outliers(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        outlier_config = self.config.get('outlier_config', [])\n",
        "        self.logger.info(f\"Starting outlier handling in '{mode}' mode.\")\n",
        "\n",
        "        for item in outlier_config:\n",
        "            column = item.get('column')\n",
        "            detection_method = item.get('detection_method')\n",
        "            treatment_method = item.get('treatment_method')\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if treatment_method != 'capping':\n",
        "                self.logger.warning(f\"Unsupported treatment method '{treatment_method}' for column '{column}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            lower_bound_key = f'{column}_{detection_method}_lower_bound'\n",
        "            upper_bound_key = f'{column}_{detection_method}_upper_bound'\n",
        "\n",
        "            try:\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if detection_method == 'iqr':\n",
        "                        iqr_multiplier = item.get('iqr_multiplier', 1.5)\n",
        "                        Q1 = df[column].quantile(0.25)\n",
        "                        Q3 = df[column].quantile(0.75)\n",
        "                        IQR = Q3 - Q1\n",
        "                        lower_bound = Q1 - iqr_multiplier * IQR\n",
        "                        upper_bound = Q3 + iqr_multiplier * IQR\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted IQR bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    elif detection_method == 'zscore':\n",
        "                        zscore_threshold = item.get('zscore_threshold', 3)\n",
        "                        mean = df[column].mean()\n",
        "                        std = df[column].std()\n",
        "                        lower_bound = mean - zscore_threshold * std\n",
        "                        upper_bound = mean + zscore_threshold * std\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted Z-score bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Unsupported detection method '{detection_method}' for column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if lower_bound_key in self.scalers and upper_bound_key in self.scalers:\n",
        "                        lower_bound = self.scalers[lower_bound_key]\n",
        "                        upper_bound = self.scalers[upper_bound_key]\n",
        "\n",
        "                        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
        "                        self.logger.info(f\"Applied capping to column '{column}' using {detection_method} bounds. Values clipped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Bounds for column '{column}' with detection method '{detection_method}' not found in scalers. Skipping transform.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during outlier handling for column '{column}' with method '{detection_method}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished outlier handling.\")\n",
        "        return df\n",
        "\n",
        "    def encode_categorical(self, X, y=None, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        categorical_encoding_config = self.config.get('categorical_encoding_config', [])\n",
        "        self.logger.info(f\"Starting categorical encoding in '{mode}' mode.\")\n",
        "\n",
        "        for item in categorical_encoding_config:\n",
        "            column = item.get('column')\n",
        "            encoding_type = item.get('encoding_type')\n",
        "            replace_original = item.get('replace_original', True) # Default to replacing original\n",
        "            params = item.get('params', {}) # For additional encoder parameters\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            # Check if the column is categorical or object type\n",
        "            if not pd.api.types.is_categorical_dtype(df[column]) and not pd.api.types.is_object_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not categorical or object type. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                encoder_key = f\"{column}_{encoding_type}\"\n",
        "\n",
        "                if encoding_type == 'label':\n",
        "                    mapping = params.get('mapping')\n",
        "                    if mapping: # Use explicit mapping if provided\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            new_column_name = f'{column}_encoded'\n",
        "                            df[new_column_name] = df[column].map(mapping)\n",
        "                            self.logger.info(f\"Applied label encoding (mapping) to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                    else: # Fallback to sklearn LabelEncoder if no explicit mapping\n",
        "                        self.logger.warning(f\"No explicit mapping provided for label encoding of column '{column}'. Using sklearn LabelEncoder.\")\n",
        "                        if mode in ['fit', 'fit_transform']:\n",
        "                            if encoder_key not in self.encoders:\n",
        "                                le = LabelEncoder()\n",
        "                                le.fit(df[column].astype(str)) # Convert to string to handle potential NaN/non-string types\n",
        "                                self.encoders[encoder_key] = le\n",
        "                                self.logger.info(f\"Fitted LabelEncoder for column '{column}'.\")\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            if encoder_key in self.encoders:\n",
        "                                new_column_name = f'{column}_encoded'\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[column].astype(str))\n",
        "                                self.logger.info(f\"Applied LabelEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                                if replace_original:\n",
        "                                    df = df.drop(columns=[column])\n",
        "                                    self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                            else:\n",
        "                                self.logger.warning(f\"No fitted LabelEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'one_hot':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, **params)\n",
        "                            ohe.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = ohe\n",
        "                            self.logger.info(f\"Fitted OneHotEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            ohe_cols = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            feature_names = self.encoders[encoder_key].get_feature_names_out([column])\n",
        "                            ohe_df = pd.DataFrame(ohe_cols, index=df.index, columns=feature_names)\n",
        "                            df = pd.concat([df, ohe_df], axis=1)\n",
        "                            self.logger.info(f\"Applied OneHotEncoder to column '{column}'. Added {len(feature_names)} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted OneHotEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'frequency':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            # Normalize=True to get proportions/frequencies\n",
        "                            freq_map = df[column].value_counts(normalize=True).to_dict()\n",
        "                            self.encoders[encoder_key] = freq_map\n",
        "                            self.logger.info(f\"Fitted frequency map for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_freq_encoded'\n",
        "                            df[new_column_name] = df[column].map(self.encoders[encoder_key])\n",
        "                            self.logger.info(f\"Applied frequency encoding to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted frequency map found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'binary':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            be = BinaryEncoder(cols=[column], handle_unknown='ignore', **params)\n",
        "                            be.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = be\n",
        "                            self.logger.info(f\"Fitted BinaryEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            binary_cols_df = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            df = pd.concat([df, binary_cols_df], axis=1)\n",
        "                            self.logger.info(f\"Applied BinaryEncoder to column '{column}'. Added {binary_cols_df.shape[1]} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted BinaryEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'target':\n",
        "                    target_column = item.get('target_column')\n",
        "                    if target_column is None:\n",
        "                        self.logger.error(f\"'target_column' must be specified for target encoding of column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if y is None:\n",
        "                            self.logger.error(f\"'y' (target variable) must be provided for target encoding in fit/fit_transform mode for column '{column}'. Skipping.\")\n",
        "                            continue\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            te = TargetEncoder(cols=[column], handle_unknown='value', **params)\n",
        "                            # Ensure y is a Series for fitting\n",
        "                            target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                            te.fit(df[[column]], target_series)\n",
        "                            self.encoders[encoder_key] = te\n",
        "                            self.logger.info(f\"Fitted TargetEncoder for column '{column}' with target '{target_column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_target_encoded'\n",
        "                            # For transform, y is optional, if provided for smoothing, use it.\n",
        "                            # If y is provided for fit_transform, it should be passed here too.\n",
        "                            if y is not None:\n",
        "                                target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]], target_series)\n",
        "                            else:\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied TargetEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted TargetEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported encoding type '{encoding_type}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during {encoding_type} encoding for column '{column}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished categorical encoding.\")\n",
        "        return df\n",
        "\n",
        "    def scale_numerical_features(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        scaling_config = self.config.get('scaling_config', [])\n",
        "        self.logger.info(f\"Starting numerical feature scaling in '{mode}' mode.\")\n",
        "\n",
        "        for item in scaling_config:\n",
        "            column = item.get('column')\n",
        "            scaling_type = item.get('scaling_type')\n",
        "            params = item.get('params', {}) # For additional scaler parameters\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping scaling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping scaling for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                scaler_key = f\"{column}_{scaling_type}\"\n",
        "\n",
        "                if scaling_type == 'standard':\n",
        "                    scaler_class = StandardScaler\n",
        "                elif scaling_type == 'minmax':\n",
        "                    scaler_class = MinMaxScaler\n",
        "                elif scaling_type == 'robust':\n",
        "                    scaler_class = RobustScaler\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported scaling type '{scaling_type}' for column '{column}'. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if scaler_key not in self.scalers:\n",
        "                        scaler = scaler_class(**params)\n",
        "                        scaler.fit(df[[column]])\n",
        "                        self.scalers[scaler_key] = scaler\n",
        "                        self.logger.info(f\"Fitted {scaling_type} scaler for column '{column}'.\")\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if scaler_key in self.scalers:\n",
        "                        df[column] = self.scalers[scaler_key].transform(df[[column]])\n",
        "                        self.logger.info(f\"Applied {scaling_type} scaling to column '{column}'.\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"No fitted {scaling_type} scaler found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during {scaling_type} scaling for column '{column}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished numerical feature scaling.\")\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        feature_engineering_config = self.config.get('feature_engineering_config', [])\n",
        "        self.logger.info(f\"Starting feature engineering in '{mode}' mode.\")\n",
        "\n",
        "        for item in feature_engineering_config:\n",
        "            feature_type = item.get('feature_type')\n",
        "            output_column = item.get('output_column')\n",
        "            input_columns = item.get('input_columns')\n",
        "            params = item.get('params', {})\n",
        "\n",
        "            if not input_columns:\n",
        "                self.logger.warning(f\"'input_columns' not provided for feature type '{feature_type}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Ensure input columns exist and are numeric where required\n",
        "            if feature_type in ['polynomial', 'log_transform', 'ratio']:\n",
        "                missing_cols = [col for col in input_columns if col not in df.columns]\n",
        "                if missing_cols:\n",
        "                    self.logger.warning(f\"Input columns {missing_cols} not found in DataFrame. Skipping '{feature_type}' feature engineering.\")\n",
        "                    continue\n",
        "\n",
        "                non_numeric_cols = [col for col in input_columns if not pd.api.types.is_numeric_dtype(df[col])]\n",
        "                if non_numeric_cols:\n",
        "                    self.logger.warning(f\"Input columns {non_numeric_cols} are not numeric. Skipping '{feature_type}' feature engineering.\")\n",
        "                    continue\n",
        "\n",
        "            try:\n",
        "                if feature_type == 'polynomial':\n",
        "                    if not output_column:\n",
        "                        self.logger.warning(f\"'output_column' not provided for polynomial feature engineering. Skipping.\")\n",
        "                        continue\n",
        "                    encoder_key = f'{output_column}_polynomial'\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            poly = PolynomialFeatures(**params)\n",
        "                            poly.fit(df[input_columns])\n",
        "                            self.encoders[encoder_key] = poly\n",
        "                            self.logger.info(f\"Fitted PolynomialFeatures for input columns '{input_columns}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            poly_features = self.encoders[encoder_key].transform(df[input_columns])\n",
        "                            feature_names = self.encoders[encoder_key].get_feature_names_out(input_columns)\n",
        "                            poly_df = pd.DataFrame(poly_features, index=df.index, columns=feature_names)\n",
        "                            df = pd.concat([df, poly_df], axis=1)\n",
        "                            self.logger.info(f\"Created {len(feature_names)} polynomial features for input columns '{input_columns}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted PolynomialFeatures found for '{input_columns}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif feature_type == 'log_transform':\n",
        "                    for col in input_columns:\n",
        "                        new_col_name = f'{col}_log'\n",
        "                        df[new_col_name] = np.log1p(df[col])\n",
        "                        self.logger.info(f\"Applied log1p transformation to column '{col}'. New column: '{new_col_name}'.\")\n",
        "\n",
        "                elif feature_type == 'ratio':\n",
        "                    if len(input_columns) != 2:\n",
        "                        self.logger.warning(f\"Ratio feature requires exactly two input columns. Got {len(input_columns)}. Skipping.\")\n",
        "                        continue\n",
        "                    col1, col2 = input_columns[0], input_columns[1]\n",
        "                    new_col_name = output_column if output_column else f'{col1}_ratio_{col2}'\n",
        "\n",
        "                    # Handle division by zero\n",
        "                    with np.errstate(divide='ignore', invalid='ignore'): # Suppress RuntimeWarning for division by zero\n",
        "                        ratio = df[col1] / df[col2]\n",
        "                        ratio.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "                    df[new_col_name] = ratio\n",
        "                    self.logger.info(f\"Created ratio feature '{new_col_name}' from '{col1}' and '{col2}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported feature type '{feature_type}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during '{feature_type}' feature engineering for input columns '{input_columns}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished feature engineering.\")\n",
        "        return df\n",
        "\n",
        "print(\"engineer_features method added to DataCleaningPipeline class.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb3d8390",
        "outputId": "03d4df20-c32b-4f98-ff39-aecbf4b6526e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fit, transform, and fit_transform methods added to DataCleaningPipeline class.\n"
          ]
        }
      ],
      "source": [
        "#Implement fit, transform, and fit_transform Methods\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders import BinaryEncoder, TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.imputers = {}\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.logger.info(\"DataCleaningPipeline initialized.\")\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        logger = logging.getLogger(self.__class__.__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "    def handle_missing_values(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        missing_values_config = self.config.get('missing_values_config', [])\n",
        "        self.logger.info(f\"Starting missing value imputation in '{mode}' mode.\")\n",
        "\n",
        "        for item in missing_values_config:\n",
        "            column = item.get('column')\n",
        "            strategy = item.get('imputation_strategy')\n",
        "            fill_value = item.get('fill_value', None)\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping imputation for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if strategy in ['mean', 'median', 'mode', 'constant']:\n",
        "                    imputer_key = f\"{column}_{strategy}\"\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if imputer_key not in self.imputers:\n",
        "                            if strategy == 'constant':\n",
        "                                if fill_value is None:\n",
        "                                    self.logger.error(f\"'fill_value' must be provided for 'constant' strategy in column '{column}'. Skipping.\")\n",
        "                                    continue\n",
        "                                imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
        "                            else:\n",
        "                                imputer = SimpleImputer(strategy=strategy)\n",
        "\n",
        "                            imputer.fit(df[[column]])\n",
        "                            self.imputers[imputer_key] = imputer\n",
        "                            self.logger.info(f\"Fitted {strategy} imputer for column '{column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if imputer_key in self.imputers:\n",
        "                            df[column] = self.imputers[imputer_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied {strategy} imputation to column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted imputer found for column '{column}' with strategy '{strategy}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif strategy in ['ffill', 'bfill']:\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if strategy == 'ffill':\n",
        "                            df[column] = df[column].ffill()\n",
        "                            self.logger.info(f\"Applied forward-fill imputation to column '{column}'.\")\n",
        "                        elif strategy == 'bfill':\n",
        "                            df[column] = df[column].bfill()\n",
        "                            self.logger.info(f\"Applied backward-fill imputation to column '{column}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported imputation strategy '{strategy}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during imputation for column '{column}' with strategy '{strategy}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished missing value imputation.\")\n",
        "        return df\n",
        "\n",
        "    def handle_outliers(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        outlier_config = self.config.get('outlier_config', [])\n",
        "        self.logger.info(f\"Starting outlier handling in '{mode}' mode.\")\n",
        "\n",
        "        for item in outlier_config:\n",
        "            column = item.get('column')\n",
        "            detection_method = item.get('detection_method')\n",
        "            treatment_method = item.get('treatment_method')\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping outlier handling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if treatment_method != 'capping':\n",
        "                self.logger.warning(f\"Unsupported treatment method '{treatment_method}' for column '{column}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            lower_bound_key = f'{column}_{detection_method}_lower_bound'\n",
        "            upper_bound_key = f'{column}_{detection_method}_upper_bound'\n",
        "\n",
        "            try:\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if detection_method == 'iqr':\n",
        "                        iqr_multiplier = item.get('iqr_multiplier', 1.5)\n",
        "                        Q1 = df[column].quantile(0.25)\n",
        "                        Q3 = df[column].quantile(0.75)\n",
        "                        IQR = Q3 - Q1\n",
        "                        lower_bound = Q1 - iqr_multiplier * IQR\n",
        "                        upper_bound = Q3 + iqr_multiplier * IQR\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted IQR bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    elif detection_method == 'zscore':\n",
        "                        zscore_threshold = item.get('zscore_threshold', 3)\n",
        "                        mean = df[column].mean()\n",
        "                        std = df[column].std()\n",
        "                        lower_bound = mean - zscore_threshold * std\n",
        "                        upper_bound = mean + zscore_threshold * std\n",
        "                        self.scalers[lower_bound_key] = lower_bound\n",
        "                        self.scalers[upper_bound_key] = upper_bound\n",
        "                        self.logger.info(f\"Fitted Z-score bounds for column '{column}': Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Unsupported detection method '{detection_method}' for column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if lower_bound_key in self.scalers and upper_bound_key in self.scalers:\n",
        "                        lower_bound = self.scalers[lower_bound_key]\n",
        "                        upper_bound = self.scalers[upper_bound_key]\n",
        "\n",
        "                        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
        "                        self.logger.info(f\"Applied capping to column '{column}' using {detection_method} bounds. Values clipped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Bounds for column '{column}' with detection method '{detection_method}' not found in scalers. Skipping transform.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during outlier handling for column '{column}' with method '{detection_method}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished outlier handling.\")\n",
        "        return df\n",
        "\n",
        "    def encode_categorical(self, X, y=None, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        categorical_encoding_config = self.config.get('categorical_encoding_config', [])\n",
        "        self.logger.info(f\"Starting categorical encoding in '{mode}' mode.\")\n",
        "\n",
        "        for item in categorical_encoding_config:\n",
        "            column = item.get('column')\n",
        "            encoding_type = item.get('encoding_type')\n",
        "            replace_original = item.get('replace_original', True) # Default to replacing original\n",
        "            params = item.get('params', {}) # For additional encoder parameters\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            # Check if the column is categorical or object type\n",
        "            if not pd.api.types.is_categorical_dtype(df[column]) and not pd.api.types.is_object_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not categorical or object type. Skipping encoding for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                encoder_key = f\"{column}_{encoding_type}\"\n",
        "\n",
        "                if encoding_type == 'label':\n",
        "                    mapping = params.get('mapping')\n",
        "                    if mapping: # Use explicit mapping if provided\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            new_column_name = f'{column}_encoded'\n",
        "                            df[new_column_name] = df[column].map(mapping)\n",
        "                            self.logger.info(f\"Applied label encoding (mapping) to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                    else: # Fallback to sklearn LabelEncoder if no explicit mapping\n",
        "                        self.logger.warning(f\"No explicit mapping provided for label encoding of column '{column}'. Using sklearn LabelEncoder.\")\n",
        "                        if mode in ['fit', 'fit_transform']:\n",
        "                            if encoder_key not in self.encoders:\n",
        "                                le = LabelEncoder()\n",
        "                                le.fit(df[column].astype(str)) # Convert to string to handle potential NaN/non-string types\n",
        "                                self.encoders[encoder_key] = le\n",
        "                                self.logger.info(f\"Fitted LabelEncoder for column '{column}'.\")\n",
        "                        if mode in ['transform', 'fit_transform']:\n",
        "                            if encoder_key in self.encoders:\n",
        "                                new_column_name = f'{column}_encoded'\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[column].astype(str))\n",
        "                                self.logger.info(f\"Applied LabelEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                                if replace_original:\n",
        "                                    df = df.drop(columns=[column])\n",
        "                                    self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                            else:\n",
        "                                self.logger.warning(f\"No fitted LabelEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'one_hot':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, **params)\n",
        "                            ohe.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = ohe\n",
        "                            self.logger.info(f\"Fitted OneHotEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            ohe_cols = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            feature_names = self.encoders[encoder_key].get_feature_names_out([column])\n",
        "                            ohe_df = pd.DataFrame(ohe_cols, index=df.index, columns=feature_names)\n",
        "                            df = pd.concat([df, ohe_df], axis=1)\n",
        "                            self.logger.info(f\"Applied OneHotEncoder to column '{column}'. Added {len(feature_names)} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted OneHotEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'frequency':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            # Normalize=True to get proportions/frequencies\n",
        "                            freq_map = df[column].value_counts(normalize=True).to_dict()\n",
        "                            self.encoders[encoder_key] = freq_map\n",
        "                            self.logger.info(f\"Fitted frequency map for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_freq_encoded'\n",
        "                            df[new_column_name] = df[column].map(self.encoders[encoder_key])\n",
        "                            self.logger.info(f\"Applied frequency encoding to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted frequency map found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'binary':\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            be = BinaryEncoder(cols=[column], handle_unknown='ignore', **params)\n",
        "                            be.fit(df[[column]])\n",
        "                            self.encoders[encoder_key] = be\n",
        "                            self.logger.info(f\"Fitted BinaryEncoder for column '{column}'.\")\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            binary_cols_df = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            df = pd.concat([df, binary_cols_df], axis=1)\n",
        "                            self.logger.info(f\"Applied BinaryEncoder to column '{column}'. Added {binary_cols_df.shape[1]} new columns.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted BinaryEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif encoding_type == 'target':\n",
        "                    target_column = item.get('target_column')\n",
        "                    if target_column is None:\n",
        "                        self.logger.error(f\"'target_column' must be specified for target encoding of column '{column}'. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if y is None:\n",
        "                            self.logger.error(f\"'y' (target variable) must be provided for target encoding in fit/fit_transform mode for column '{column}'. Skipping.\")\n",
        "                            continue\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            te = TargetEncoder(cols=[column], handle_unknown='value', **params)\n",
        "                            # Ensure y is a Series for fitting\n",
        "                            target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                            te.fit(df[[column]], target_series)\n",
        "                            self.encoders[encoder_key] = te\n",
        "                            self.logger.info(f\"Fitted TargetEncoder for column '{column}' with target '{target_column}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            new_column_name = f'{column}_target_encoded'\n",
        "                            # For transform, y is optional, if provided for smoothing, use it.\n",
        "                            # If y is provided for fit_transform, it should be passed here too.\n",
        "                            if y is not None:\n",
        "                                target_series = y[target_column] if isinstance(y, pd.DataFrame) else y\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]], target_series)\n",
        "                            else:\n",
        "                                df[new_column_name] = self.encoders[encoder_key].transform(df[[column]])\n",
        "                            self.logger.info(f\"Applied TargetEncoder to column '{column}'. New column: '{new_column_name}'.\")\n",
        "                            if replace_original:\n",
        "                                df = df.drop(columns=[column])\n",
        "                                self.logger.info(f\"Dropped original column '{column}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted TargetEncoder found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported encoding type '{encoding_type}' for column '{column}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during {encoding_type} encoding for column '{column}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished categorical encoding.\")\n",
        "        return df\n",
        "\n",
        "    def scale_numerical_features(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        scaling_config = self.config.get('scaling_config', [])\n",
        "        self.logger.info(f\"Starting numerical feature scaling in '{mode}' mode.\")\n",
        "\n",
        "        for item in scaling_config:\n",
        "            column = item.get('column')\n",
        "            scaling_type = item.get('scaling_type')\n",
        "            params = item.get('params', {}) # For additional scaler parameters\n",
        "\n",
        "            if column not in df.columns:\n",
        "                self.logger.warning(f\"Column '{column}' not found in DataFrame. Skipping scaling for this column.\")\n",
        "                continue\n",
        "\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                self.logger.warning(f\"Column '{column}' is not numeric. Skipping scaling for this column.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                scaler_key = f\"{column}_{scaling_type}\"\n",
        "\n",
        "                if scaling_type == 'standard':\n",
        "                    scaler_class = StandardScaler\n",
        "                elif scaling_type == 'minmax':\n",
        "                    scaler_class = MinMaxScaler\n",
        "                elif scaling_type == 'robust':\n",
        "                    scaler_class = RobustScaler\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported scaling type '{scaling_type}' for column '{column}'. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                if mode in ['fit', 'fit_transform']:\n",
        "                    if scaler_key not in self.scalers:\n",
        "                        scaler = scaler_class(**params)\n",
        "                        scaler.fit(df[[column]])\n",
        "                        self.scalers[scaler_key] = scaler\n",
        "                        self.logger.info(f\"Fitted {scaling_type} scaler for column '{column}'.\")\n",
        "\n",
        "                if mode in ['transform', 'fit_transform']:\n",
        "                    if scaler_key in self.scalers:\n",
        "                        df[column] = self.scalers[scaler_key].transform(df[[column]])\n",
        "                        self.logger.info(f\"Applied {scaling_type} scaling to column '{column}'.\")\n",
        "                    else:\n",
        "                        self.logger.warning(f\"No fitted {scaling_type} scaler found for column '{column}' in transform mode. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during {scaling_type} scaling for column '{column}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished numerical feature scaling.\")\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, X, mode='fit_transform'):\n",
        "        df = X.copy(deep=True)\n",
        "        feature_engineering_config = self.config.get('feature_engineering_config', [])\n",
        "        self.logger.info(f\"Starting feature engineering in '{mode}' mode.\")\n",
        "\n",
        "        for item in feature_engineering_config:\n",
        "            feature_type = item.get('feature_type')\n",
        "            output_column = item.get('output_column')\n",
        "            input_columns = item.get('input_columns')\n",
        "            params = item.get('params', {})\n",
        "\n",
        "            if not input_columns:\n",
        "                self.logger.warning(f\"'input_columns' not provided for feature type '{feature_type}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Ensure input columns exist and are numeric where required\n",
        "            if feature_type in ['polynomial', 'log_transform', 'ratio']:\n",
        "                missing_cols = [col for col in input_columns if col not in df.columns]\n",
        "                if missing_cols:\n",
        "                    self.logger.warning(f\"Input columns {missing_cols} not found in DataFrame. Skipping '{feature_type}' feature engineering.\")\n",
        "                    continue\n",
        "\n",
        "                non_numeric_cols = [col for col in input_columns if not pd.api.types.is_numeric_dtype(df[col])]\n",
        "                if non_numeric_cols:\n",
        "                    self.logger.warning(f\"Input columns {non_numeric_cols} are not numeric. Skipping '{feature_type}' feature engineering.\")\n",
        "                    continue\n",
        "\n",
        "            try:\n",
        "                if feature_type == 'polynomial':\n",
        "                    if not output_column:\n",
        "                        self.logger.warning(f\"'output_column' not provided for polynomial feature engineering. Skipping.\")\n",
        "                        continue\n",
        "                    encoder_key = f'{output_column}_polynomial'\n",
        "\n",
        "                    if mode in ['fit', 'fit_transform']:\n",
        "                        if encoder_key not in self.encoders:\n",
        "                            poly = PolynomialFeatures(**params)\n",
        "                            poly.fit(df[input_columns])\n",
        "                            self.encoders[encoder_key] = poly\n",
        "                            self.logger.info(f\"Fitted PolynomialFeatures for input columns '{input_columns}'.\")\n",
        "\n",
        "                    if mode in ['transform', 'fit_transform']:\n",
        "                        if encoder_key in self.encoders:\n",
        "                            poly_features = self.encoders[encoder_key].transform(df[input_columns])\n",
        "                            feature_names = self.encoders[encoder_key].get_feature_names_out(input_columns)\n",
        "                            poly_df = pd.DataFrame(poly_features, index=df.index, columns=feature_names)\n",
        "                            df = pd.concat([df, poly_df], axis=1)\n",
        "                            self.logger.info(f\"Created {len(feature_names)} polynomial features for input columns '{input_columns}'.\")\n",
        "                        else:\n",
        "                            self.logger.warning(f\"No fitted PolynomialFeatures found for '{input_columns}' in transform mode. Skipping.\")\n",
        "\n",
        "                elif feature_type == 'log_transform':\n",
        "                    for col in input_columns:\n",
        "                        new_col_name = f'{col}_log'\n",
        "                        # Handle potential non-positive values for log transform\n",
        "                        df[new_col_name] = np.log1p(df[col].fillna(0))\n",
        "                        self.logger.info(f\"Applied log1p transformation to column '{col}'. New column: '{new_col_name}'.\")\n",
        "\n",
        "                elif feature_type == 'ratio':\n",
        "                    if len(input_columns) != 2: # Ratio requires two columns\n",
        "                        self.logger.warning(f\"Ratio feature requires exactly two input columns. Got {len(input_columns)}. Skipping.\")\n",
        "                        continue\n",
        "                    col1, col2 = input_columns[0], input_columns[1]\n",
        "                    new_col_name = output_column if output_column else f'{col1}_ratio_{col2}'\n",
        "\n",
        "                    # Handle division by zero\n",
        "                    with np.errstate(divide='ignore', invalid='ignore'): # Suppress RuntimeWarning for division by zero\n",
        "                        ratio = df[col1] / df[col2]\n",
        "                        ratio.replace([np.inf, -np.inf], np.nan, inplace=True) # Replace inf with NaN\n",
        "                    df[new_col_name] = ratio\n",
        "                    self.logger.info(f\"Created ratio feature '{new_col_name}' from '{col1}' and '{col2}'.\")\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unsupported feature type '{feature_type}'. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during '{feature_type}' feature engineering for input columns '{input_columns}': {e}\")\n",
        "\n",
        "        self.logger.info(\"Finished feature engineering.\")\n",
        "        return df\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.logger.info(\"Starting fit process.\")\n",
        "        df = X.copy(deep=True)\n",
        "\n",
        "        # Order of operations (can be configured in self.config if needed)\n",
        "        processing_steps = [\n",
        "            ('handle_missing_values', self.handle_missing_values),\n",
        "            ('handle_outliers', self.handle_outliers),\n",
        "            ('encode_categorical', self.encode_categorical),\n",
        "            ('engineer_features', self.engineer_features),\n",
        "            ('scale_numerical_features', self.scale_numerical_features)\n",
        "        ]\n",
        "\n",
        "        for step_name, step_func in processing_steps:\n",
        "            try:\n",
        "                if step_name == 'encode_categorical':\n",
        "                    df = step_func(df, y=y, mode='fit')\n",
        "                else:\n",
        "                    df = step_func(df, mode='fit')\n",
        "                self.logger.info(f\"Successfully executed fit step: {step_name}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during fit step '{step_name}': {e}\")\n",
        "                raise # Re-raise to stop pipeline if fit fails critically\n",
        "\n",
        "        self.logger.info(\"Fit process completed successfully.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        self.logger.info(\"Starting transform process.\")\n",
        "        df = X.copy(deep=True)\n",
        "\n",
        "        # Order of operations (must be same as fit)\n",
        "        processing_steps = [\n",
        "            ('handle_missing_values', self.handle_missing_values),\n",
        "            ('handle_outliers', self.handle_outliers),\n",
        "            ('encode_categorical', self.encode_categorical),\n",
        "            ('engineer_features', self.engineer_features),\n",
        "            ('scale_numerical_features', self.scale_numerical_features)\n",
        "        ]\n",
        "\n",
        "        for step_name, step_func in processing_steps:\n",
        "            try:\n",
        "                if step_name == 'encode_categorical':\n",
        "                    df = step_func(df, y=y, mode='transform')\n",
        "                else:\n",
        "                    df = step_func(df, mode='transform')\n",
        "                self.logger.info(f\"Successfully executed transform step: {step_name}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error during transform step '{step_name}': {e}\")\n",
        "                raise # Re-raise to stop pipeline if transform fails critically\n",
        "\n",
        "        self.logger.info(\"Transform process completed successfully.\")\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.logger.info(\"Starting fit_transform process.\")\n",
        "        # Call fit to learn parameters\n",
        "        self.fit(X, y)\n",
        "        # Call transform to apply learned parameters\n",
        "        transformed_df = self.transform(X, y)\n",
        "        self.logger.info(\"Fit_transform process completed successfully.\")\n",
        "        return transformed_df\n",
        "\n",
        "print(\"fit, transform, and fit_transform methods added to DataCleaningPipeline class.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DISCUSSION:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lab session, we learned data wrangling and transformation using python and python libraries like pandas, numpy, sklearn, matplotlib, etc.\n",
        "This lab provided practical knowledge of advanced data wrangling and transformation techniques. It highlighted the importance of preprocessing in data analysis and machine learning. Clean and well-structured data improves accuracy, efficiency, and decision-making. Advanced wrangling tools and techniques are essential skills for data analysts and data scientists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONCLUSION:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, we successfully perform tasks related to data wrangling and transformation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
